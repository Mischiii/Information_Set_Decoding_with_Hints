{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- README --- #\n",
    "\n",
    "# The Jupyter Notebook contains the practical Implementation of the hints and techniques mentioned in\n",
    "# Information-Set Decoding with Hints by Anna-Lena Horlemann et. al (https://eprint.iacr.org/2021/279.pdf)\n",
    "# and further attempts, techniques and hints, that I examined during the work for my Bachelor's Thesis.\n",
    "\n",
    "# Disclamer: To prevent code duplications, the cells that are further down build on the upper ones.\n",
    "#            This makes it necessary to partially execute the upper cells before using the lower ones.\n",
    "\n",
    "# Included in this Jupyter Notebook are the following things:\n",
    "# - Utility Functions for Partial Gaussian Eliminations, Approximation of the Distance of the Code by using the GV-Bound and more\n",
    "# - A Generator for the Syndrome Decoding Problem (SDP), that uses the approximated distance to generate SDP Instances \n",
    "# - The ISD Algorithms of Prange, Lee-Brickell, Dumer-Stern and MMT (May, Meurer, Thomae)\n",
    "# - The corresponding runtime and memory complexity of these ISD Algorithms\n",
    "# - A flag, that can be set to generate the most efficient parameters for the ISD Algorithms Lee-Brickell, Stern, Dumer-Stern and MMT \n",
    "# - A Hints Generator, that generate of each mentioned Hint Type multiple instances at once \n",
    "# - The SDP Transformations, to generate smaller Instances of the SDP, mentioned in Information-Set Decoding with Hints\n",
    "# - A Examination of Changes in runtime of Prange, when changing the parameters like in the Transformations and in general\n",
    "# - The Modified Prange Algorithm with the mentioned Optimization Technique of Anna-Lena Horlemann et. al\n",
    "# - The Modified Lee-Brickell Algorithm with the mentioned Optimization Technique of Anna-Lena Horlemann et. al\n",
    "# - The Modified Stern Algorithm with the mentioned Optimization Technique of Anna-Lena Horlemann et. al\n",
    "# - A Optimization for the SDP Transformation of Hint Type Six for Processing multiple hints at once\n",
    "# - A Attempt for a stronger Transformation through multiple Hints of Hint Type Six \n",
    "# - A Hints Generator, that generate variants of the mentioned Hint Types; Also here multiple instances can be computed at once\n",
    "# - Introducing a new Hint Type \"Approximated Hints\"\n",
    "# - Implementing the BKW Algorithm optimizing the potential of the \"Approximated Hints\"\n",
    "# - List Decoding for the Syndrome Decoding Problem\n",
    "# - Attempts of Guessing Error Locations without using the Transformations + Success Probabilities and Resulting Runtimes\n",
    "# - Attempts of Guessing Error Locations with using the Transformations + Success Probabilities and Resulting Runtimes\n",
    "# - Attempts of Guessing Error-Free Locations with using the Transformations + Success Probabilities and Resulting Runtimes\n",
    "# - Attempts Guessing Sub-Errors and their Hamming Weight + Success Probabilities and Resulting Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- UTILITY FUNCTIONS --- #\n",
    "\n",
    "import numpy as np \n",
    "import copy\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "from itertools import combinations\n",
    "\n",
    "# Input: Matrix A ϵ GF(2)^(m x m)\n",
    "# Output: is_invertable, inverted_matrix\n",
    "#         If the matrix is invertable is_invertable is true and inverted_matrix is A^(-1)\n",
    "#         If the matrix is not invertable is_invertable is false and invertable_matrix is []\n",
    "def invert_matrix(A):\n",
    "    dimension = A.nrows()\n",
    "    \n",
    "    if A.rank() != dimension:\n",
    "        return False, []\n",
    "    \n",
    "    I = identity_matrix(dimension)\n",
    "    A = A.augment(I)\n",
    "    A = A.echelon_form()\n",
    "    return True, A[:,dimension:]\n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G ϵ GF(2)^(k x n)\n",
    "# Output: Parity-Check Matrix P ϵ GF(2)^((n-k) x n)\n",
    "def from_generator_to_parity_check(G):\n",
    "    C = LinearCode(G)\n",
    "    return C.parity_check_matrix()\n",
    "\n",
    "\n",
    "\n",
    "# Input: Parity-Check Matrix P ϵ GF(2)^((n-k) x n)\n",
    "# Input: Generator Matrix G ϵ GF(2)^(k x n)\n",
    "def from_parity_check_to_generator(P):\n",
    "    C_dual = LinearCode(P)\n",
    "    return C_dual.parity_check_matrix()\n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: A Errornous Codeword r = mG + e and Parity-Check Matrix P ϵ GF(2)^((n-k) x n)\n",
    "# Output: Syndrome s\n",
    "def from_errornous_codeword_to_syndrome(P, r):\n",
    "    return P * r\n",
    "\n",
    "\n",
    "\n",
    "# Input: Syndrome s and Parity-Check Matrix P ϵ GF(2)^((n-k) x n)   \n",
    "# Output: A Errornous Codeword r = m'G + e   \n",
    "def from_syndrome_to_errornous_codeword(P, s, k):\n",
    "    P_copy = copy.copy(P)\n",
    "    P_copy = P_copy.augment(s)\n",
    "    P_copy.echelon_form()\n",
    "    r_part = P_copy.columns()[-1]\n",
    "    return vector(GF(2), chain(r_part, vector(GF(2), [0]*k)))\n",
    "    \n",
    "\n",
    "\n",
    "# Input: Dimension of the wanted Permutation Matrix (n x n)\n",
    "# Output: Random Permutation Matrix of Dimension (n x n)\n",
    "def permutation_matrix(dimension):\n",
    "    permutation = np.random.permutation(int(dimension)) \n",
    "    H = zero_matrix(GF(2), dimension)\n",
    "    for index in range(0,dimension):\n",
    "        H[permutation[index],index] = 1\n",
    "    return H \n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: Dimension of the vectors in GF(2) and the amount of ones \n",
    "# Output: All unique permutations in GF(2)^size with the stated number of ones \n",
    "def unique_permutations(size, ones):\n",
    "    def bit_patterns(size, ones):\n",
    "        for pos in map(set, combinations(range(size), ones)):\n",
    "            yield [int(i in pos) for i in range(size)]\n",
    "     \n",
    "    permutations = []\n",
    "    for element in bit_patterns(size, ones):\n",
    "        permutations.append(vector(GF(2), element))\n",
    "        \n",
    "    return permutations \n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimension of the output vectors in GF(2), the amount of ones and a values pool, where we can place the ones\n",
    "# Output: All unique permutations in GF(2)^size with the stated number of ones and ones only in the values pool \n",
    "def unique_combinations(values, size, ones):\n",
    "    def bit_patterns(size, ones):\n",
    "        for pos in map(set, combinations(range(size), ones)):\n",
    "            yield [int(i in pos) for i in range(size)]\n",
    "     \n",
    "    permutations = []\n",
    "    for element in bit_patterns(len(values), ones):\n",
    "        new_vector = vector(GF(2), size)\n",
    "        counter = 0 \n",
    "        for index in values:\n",
    "            new_vector[index] = element[counter]\n",
    "            counter += 1 \n",
    "        \n",
    "        permutations.append(new_vector)\n",
    "        \n",
    "    return permutations \n",
    "    \n",
    "\n",
    "\n",
    "# Input: Parity-Check Matrix P of the Dimension (n-k x n) and Leon's l window\n",
    "# Output: is_computable, semi_systematic_P, gaussian_opeations \n",
    "#         If the matrix is not computable is_computable is true and semi_systematic_P is P in semi-systematic form \n",
    "#         If the matrix is not computable is_computable is false and semi_systematic_P is []\n",
    "#         gaussian_operations is a Matrix G, that holds all Gaussian Operations, that have be done \n",
    "def compute_semi_systematic_form(P, n, k, l):\n",
    "    if rank(P) <= n-k-l:\n",
    "        return False, [], []\n",
    "    \n",
    "    G = identity_matrix(GF(2), n-k)\n",
    "    for current_row in range(0,n-k-l):\n",
    "        new_up = 0 \n",
    "        for candidate in range(current_row, n-k):\n",
    "            if P[candidate, current_row] == 1:\n",
    "                new_up = candidate \n",
    "                break\n",
    "\n",
    "        if new_up != current_row: \n",
    "            P.swap_rows(new_up, current_row)\n",
    "            G.swap_rows(new_up, current_row)\n",
    "            \n",
    "        for delete in range(0, n-k):\n",
    "            if delete == current_row: \n",
    "                continue\n",
    "            \n",
    "            if P[delete, current_row] == 1: \n",
    "                P[delete] = P[delete] + P[current_row]\n",
    "                G[delete] = G[delete] + G[current_row] \n",
    "    \n",
    "    semi_systematic_P = P\n",
    "    gaussian_operations = G\n",
    "    \n",
    "    return True, semi_systematic_P, gaussian_operations\n",
    "\n",
    "\n",
    "\n",
    "# Input: Length of the vector and amount_of_vectors\n",
    "# Output: amount_of_vectors random measurement vectors v_i GF(2)^length\\{0}, that are linearly independent \n",
    "def generate_linearly_independent_v(length, amount_of_vectors):\n",
    "    vectors = []\n",
    "    check_up = []\n",
    "    for index in range(1, amount_of_vectors+1):\n",
    "        check_up = copy.copy(vectors)\n",
    "        while True:\n",
    "            amount_ones = np.random.randint(1,length)\n",
    "            sample_v = [0]*int(length-amount_ones) + [1]*amount_ones\n",
    "            v = vector(GF(2), sample_v)\n",
    "            shuffle(v)\n",
    "            check_up.append(v)\n",
    "            \n",
    "            check_up_matrix = matrix(GF(2), check_up)\n",
    "            if rank(check_up_matrix) == index:\n",
    "                vectors.append(v)\n",
    "                break \n",
    "       \n",
    "    return vectors\n",
    "\n",
    "\n",
    "\n",
    "# Input: Parity-Check Matrix P of the Dimensions (n-k) x n and amount_of_vectors\n",
    "# Output: amount_of_vectors measurement vectors v_i ϵ GF(2)^n\\{0}, that are linearly independent together with the rows of P as Array \n",
    "def generate_linearly_independent_v_with_P(P, n, k, amount_of_vectors):\n",
    "    vectors = []\n",
    "    updated_P = copy.copy(P)\n",
    "    check_up_P = copy.copy(P)\n",
    "    \n",
    "    for index in range(1, amount_of_vectors+1):\n",
    "        while True:\n",
    "            amount_ones = np.random.randint(1,n)\n",
    "            sample_v = [0]*int(n-amount_ones) + [1]*amount_ones \n",
    "            v = vector(GF(2), sample_v)\n",
    "            shuffle(v)\n",
    "            \n",
    "            check_up_P = copy.copy(updated_P)\n",
    "            check_up_P = check_up_P.transpose()\n",
    "            check_up_P = check_up_P.augment(v)\n",
    "            check_up_P = check_up_P.transpose()\n",
    "            \n",
    "            if rank(check_up_P) == n-k+index:\n",
    "                vectors.append(v)\n",
    "                updated_P = copy.copy(check_up_P)\n",
    "                break \n",
    "                \n",
    "    return vectors\n",
    "\n",
    "\n",
    "    \n",
    "# Input: A vector v ϵ GF(2)^k\\{0} and position x \n",
    "# Output: A invertable matrix A ϵ GF(2)^(k x k), so that A * v = d_(x), where d_x is the unit vector \n",
    "#         >> MANDATORY FOR TRANSFORMATION OF HINT TYPE 6\n",
    "# Disclamer: Very naive Implmentation, by using the randomness of random_matrix()\n",
    "def generate_matrix_A(v, k, x):\n",
    "    while True:\n",
    "        if len(v) != k:\n",
    "            return \"ERROR!\"\n",
    "        \n",
    "        A = random_matrix(GF(2), k, k)\n",
    "       \n",
    "        for row in range(k):\n",
    "            # Checking & Modifiying the Matrix in the Rows, that are not x\n",
    "            if row != x:\n",
    "                current_row_tuple = A[row, :]\n",
    "                current_row = vector(GF(2), [0]*k)\n",
    "                for index in range(k):\n",
    "                    current_row[index] = current_row_tuple[0][index]\n",
    "                \n",
    "                if v.dot_product(current_row) == 0:\n",
    "                    continue\n",
    "                \n",
    "                for column in range(k):\n",
    "                    if A[row, column] == 1 and v[column] == 1:\n",
    "                        A[row, column] = 0 \n",
    "                        break \n",
    "                \n",
    "            else: \n",
    "                # Checking & Modifying the Matrix in the Row x\n",
    "                x_row_tuple = A[x, :]\n",
    "                x_row = vector(GF(2), [0]*k)\n",
    "                for index in range(k):\n",
    "                    x_row[index] = x_row_tuple[0][index]\n",
    "        \n",
    "                if v.dot_product(x_row) == 0:\n",
    "                    for column in range(k):\n",
    "                        if A[x, column] == 0 and v[column] == 1:\n",
    "                            A[x, column] = 1\n",
    "                            break \n",
    "                        \n",
    "        if rank(A) == k:\n",
    "            return A \n",
    "\n",
    "        \n",
    "        \n",
    "# Input: The Size of the Set, that should be divided into amount_partitions almost equal size partitions\n",
    "# Output: Cardinalities of the Partitions        \n",
    "def create_equal_partition(amount_partitions, n):\n",
    "    W = list(np.arange(n))\n",
    "    partitions = np.array_split(W, amount_partitions)  \n",
    "    cardinalities = []\n",
    "    for index in range(amount_partitions):\n",
    "        cardinalities.append(len(partitions[index]))\n",
    "        \n",
    "    return cardinalities\n",
    "\n",
    "\n",
    "        \n",
    "# Input: The amount_partitions and the range of values delimited by n so range(0, n)        \n",
    "# Output: Partitions Array, that holds the cardinality of the partitions        \n",
    "def create_random_partition(amount_partitions, n):\n",
    "    while True:\n",
    "        partitions = []\n",
    "        for index in range(amount_partitions):\n",
    "            partitions.append(np.random.randint(1, n))\n",
    "        \n",
    "        in_total = sum(partitions)\n",
    "        \n",
    "        if in_total < n: \n",
    "            continue \n",
    "            \n",
    "        while in_total > n:\n",
    "            random_index = np.random.randint(0, amount_partitions)\n",
    "            \n",
    "            if partitions[random_index] == 1:\n",
    "                continue\n",
    "            \n",
    "            partitions[random_index] = partitions[random_index] - 1\n",
    "            in_total = in_total - 1\n",
    "            \n",
    "        return partitions\n",
    "    \n",
    "\n",
    "    \n",
    "# Input: A value target, that determines how large the sum should be and the length l of the sum\n",
    "# Output: Returns a array with all possible lists of size l, that have as sum target\n",
    "def all_possible_sums(target, l):\n",
    "    def all_possible_sums_rec(target, current_sum, start, output, result):\n",
    "        if current_sum == target:\n",
    "            output.append(copy.copy(result))\n",
    "\n",
    "        for i in range(start, target):\n",
    "            temp_sum = current_sum + i\n",
    "            if temp_sum <= target:\n",
    "                result.append(i)\n",
    "                all_possible_sums_rec(target, temp_sum, i, output, result)\n",
    "                result.pop()\n",
    "            else:\n",
    "                return\n",
    "    \n",
    "    if target < 1:\n",
    "        return None\n",
    "    \n",
    "    if target == 1:\n",
    "        permutations = unique_permutations(l, target)\n",
    "        all_possible_sums_length_l = []\n",
    "        for permutation in permutations:\n",
    "            all_possible_sums_length_l.append(list(permutation))\n",
    "            \n",
    "        return all_possible_sums_length_l    \n",
    "    \n",
    "    sums = []\n",
    "    all_possible_sums_rec(target, 0, 1, sums, [])\n",
    "    sums_length_l = []\n",
    "    \n",
    "    for candidate in sums:\n",
    "        if len(candidate) > l:\n",
    "            continue\n",
    "         \n",
    "        if len(candidate) == l:\n",
    "            sums_length_l.append(candidate)\n",
    "        else:\n",
    "            zero_list = [0]*(l-len(candidate))\n",
    "            sums_length_l.append(candidate + zero_list)\n",
    "        \n",
    "    all_possible_sums_length_l = []\n",
    "    for sum_l in sums_length_l:\n",
    "         all_possible_sums_length_l += list(multiset_permutations(sum_l))\n",
    "            \n",
    "    return all_possible_sums_length_l\n",
    "\n",
    "\n",
    "\n",
    "# Input: Value p between 0 and 1 \n",
    "# Output: Binary Entropy of p \n",
    "def binary_entropy(p):\n",
    "    if p < 0 or p > 1:\n",
    "        return \"p has to be between 0 and 1!\"\n",
    "    \n",
    "    if p == 0 or p == 1:\n",
    "        return 0 \n",
    "\n",
    "    return float((-p)*log(p, 2) - (1-p)*log(1-p, 2))\n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: Parameters n and k of the Linear Code\n",
    "# Output: Approximated Distance by the GV Bound \n",
    "def approximate_distance(n,k):    \n",
    "    d = 0\n",
    "    boundary_value = float(1 - k/n)\n",
    "    while True:\n",
    "        if d == n:\n",
    "            return n\n",
    "        \n",
    "        current_binary_entropy = binary_entropy(float(d/n))\n",
    "        # Since the Distance Approximation is not precise by the GV-Bound we use a factor of 1.027 to \n",
    "        # ensure, that this approximation satisfy, that the smallest McEliece Instance n = 3488, k = 2720, w = 64 is unique decodable!\n",
    "        if current_binary_entropy > boundary_value * 1.033:\n",
    "            return d \n",
    "        else:\n",
    "            d += 1 \n",
    "    \n",
    "\n",
    "    \n",
    "# Input: Parameters n and k of the Linear Code     \n",
    "# Output: Approximated Packing Radius through GV-Bound\n",
    "def approximated_packing_radius(n,k):\n",
    "    d = approximate_distance(n,k)\n",
    "    if d == 0:\n",
    "        return 0 \n",
    "    packing_radius = floor((d-1)/2)\n",
    "    return packing_radius\n",
    "\n",
    "\n",
    "\n",
    "# Input: Parameters n, k and w = wt_H(e)\n",
    "# Output: Estimating whether Decoding is unique through GV-Bound\n",
    "def is_unique_decoding(n, k, w):\n",
    "    estimated_w = approximated_packing_radius(n,k)\n",
    "    if estimated_w >= w:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Input: N \"Grundgesamtheit\", M the amount of successes, n the amount of samples and k the expected amount of successes\n",
    "# Output: P(X = k) for the hypergeometric distribution\n",
    "def hypergeometric_distribution(N, M, n, k):\n",
    "    nominator = binomial(M, k) * binomial(N-M, n-k)\n",
    "    denominator = binomial(N, n)\n",
    "    return nominator/denominator \n",
    "\n",
    "\n",
    "# Input: N \"Grundgesamtheit\", M the amount of successes, n the amount of samples and k the expected amount of successes\n",
    "# Output: P(X < k) for the hypergeometric distribution\n",
    "def cumulative_hypergeometric_distribution(N, M, n, k):\n",
    "    probability = 0\n",
    "    for index in range(k):\n",
    "        probability += hypergeometric_distribution(N, M, n, index)\n",
    "        \n",
    "    return probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- RUNTIME AND SPACE COMPLEXITIES --- #\n",
    "\n",
    "# Input: Dimensions n,k and w(e)= w     \n",
    "# Output: Resulting Runtime Complexity of Prange's ISD algorithm \n",
    "def runtime_complexity_prange(n, k, w):\n",
    "    return binomial(n, w) / binomial(n-k, w)\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions n,k and w(e)= w     \n",
    "# Output: Resulting Logarithmic to the Base of 2 Runtime Complexity of Prange's ISD algorithm \n",
    "def logarithmic_runtime_complexity_prange(n, k, w):\n",
    "    return ceil(binary_entropy(w/n) * n - binary_entropy(w/(n-k)) * (n-k))\n",
    "    \n",
    "\n",
    "    \n",
    "# Input: Dimensions n,k , outside weight p with p<w and w(e)= w\n",
    "# Output: Resulting Runtime Complexity of Lee-Brickell ISD \n",
    "def runtime_complexity_lee_brickell(n, k, w, p):\n",
    "    return binomial(n, w) / binomial(n-k, w-p)\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions n,k , inside weight p with 2p <= w, w(e)= w, Leon's l window\n",
    "# Output: Resulting Runtime Complexity of Stern ISD \n",
    "def runtime_complexity_isd_stern(n, k, w, p, l):\n",
    "    dividend = binomial(n, w)*max(1, binomial(floor(k/2), p)**2 *pow(2, -l))\n",
    "    divisor = binomial(n-k-l, w-2*p) * binomial(floor(k/2), p)\n",
    "    if divisor == 0:\n",
    "        return None \n",
    "    \n",
    "    return dividend / divisor\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimension k and inside weight p\n",
    "# Output: Resulting Memory Complexity of Stern ISD\n",
    "def memory_complexity_isd_stern(k, p):\n",
    "    return binomial(floor(k/2), p)\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions n,k , outside weight p with p<w, w(e)= w, Leon's l window\n",
    "# Output: Resulting Runtime Complexity of Dumer-Stern ISD \n",
    "def runtime_complexity_dumer_stern(n, k, w, p, l):\n",
    "    dividend = binomial(n, w)*max(1, binomial((k+l)/2, p/2)*pow(2, -l))\n",
    "    divisor = binomial(n-k-l, w-p) * binomial((k+l)/2, p/2)\n",
    "    if divisor == 0:\n",
    "        return None \n",
    "    \n",
    "    return dividend / divisor\n",
    "                    \n",
    "\n",
    "    \n",
    "# Input: Dimension k, outside weight p and Leon's l window\n",
    "# Output: Resulting Memory Complexity of Dumer-Stern ISD \n",
    "def memory_complexity_dumer_stern(k, p, l):\n",
    "    return binomial((k+l)/2, p/2)\n",
    "    \n",
    "\n",
    "    \n",
    "# Input: Dimensions n,k , outside weight p with p<w, w(e)= w, Leon's l window\n",
    "# Output: Resulting Runtime Complexity of MMT ISD    \n",
    "def runtime_complexity_mmt(n, k, w, p, l):\n",
    "    dividend = binomial(n, w) * binomial(k+l, p/2) * max(1, binomial(k+l, p/2) * pow(2, l-p) / binomial(p, p/2))\n",
    "    divisor = binomial(n-k-l, w-p) * binomial(k+l, p) * binomial(p, p/2)\n",
    "    if divisor == 0:\n",
    "        return None \n",
    "    \n",
    "    return dividend / divisor \n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions k, outside weight p and Leon's l window\n",
    "# Output: Resulting Memory Complexity of MMT ISD \n",
    "def memory_complexity_mmt(k, p, l):\n",
    "    return binomial(k+l, p/2) / binomial(p, p/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- CREATING A INSTANCE OF THE SYNDROME DECODING PROBLEM (SDP) --- #\n",
    "\n",
    "from numpy import random \n",
    "\n",
    "# Input: Dimensions n,k and the hamming weight of the error notated w \n",
    "# Output: Parity Check Matrix P, Generator Matrix G, Syndrome s, Message r = mG+e, the Error e and m \n",
    "# >> We need there everything to generate hints and having the possibility to switch between the two forms of the SDP\n",
    "#    First Form - Given here is P and s\n",
    "#    Second Form - Given here is G and r \n",
    "def generate_syndrome_decoding_problem(n, k, w):\n",
    "    G = zero_matrix(GF(2), n, k)\n",
    "    while G.rank() != k:\n",
    "        G = random_matrix(GF(2), k, n)\n",
    "        \n",
    "    C = LinearCode(G)\n",
    "    P = C.parity_check_matrix()\n",
    "    \n",
    "    amount_ones = random.randint(1, k)\n",
    "    sample_m = [0]*int(k-amount_ones) + [1]*int(amount_ones)\n",
    "    m = vector(GF(2), sample_m)\n",
    "    shuffle(m)\n",
    "    c = C.encode(m)\n",
    "    \n",
    "    sample_e = [0]*int(n-w) + [1]*int(w)\n",
    "    e = vector(GF(2), sample_e)\n",
    "    shuffle(e)\n",
    "    s = P * e\n",
    "    r = vector(GF(2), c + e)\n",
    "    \n",
    "    return P, G, s, r, e, m \n",
    "\n",
    "\n",
    "\n",
    "# --- USAGE --- #\n",
    "n = 237\n",
    "k = 124\n",
    "w = approximated_packing_radius(n,k)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION OF THE INFORMATION-SET DECODING ALGORITHM OF PRANGE --- #    \n",
    "\n",
    "import random\n",
    "from itertools import chain\n",
    " \n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n,k and w(e)= w \n",
    "# Output: Error Vector e  \n",
    "def isd_primal_prange(G, r, n, k, w):\n",
    "    success = False\n",
    "    \n",
    "    while not success:\n",
    "        I = sorted(random.sample(list(range(n)), k))\n",
    "    \n",
    "        if G[:, I].rank() != k:\n",
    "            continue \n",
    "\n",
    "        G_I = G[:, I]\n",
    "        G_I_inverse = G_I.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_I_inverse * G\n",
    "        calculated_e = r + r_modified * G_new\n",
    "                \n",
    "        if calculated_e.hamming_weight() == w:\n",
    "            return calculated_e  \n",
    "        \n",
    "        \n",
    "\n",
    "# Input: Parity-Check Matrix P, Syndrome s, Dimensions n,k and w(e)= w \n",
    "# Output: Error e\n",
    "def isd_dual_prange(P, s, n, k, w):\n",
    "    finished_step1 = False\n",
    "    \n",
    "    while not finished_step1:\n",
    "        finished_step2 = False\n",
    "        \n",
    "        while not finished_step2:\n",
    "            H = permutation_matrix(n)\n",
    "            PH = P * H\n",
    "            P1 = PH[:,:int(n-k)]\n",
    "            is_invertable, P1_inverted = invert_matrix(P1)\n",
    "            \n",
    "            if is_invertable:\n",
    "                finished_step2 = True\n",
    "                \n",
    "        e1 = P1_inverted * s\n",
    "        if e1.hamming_weight() == w:\n",
    "            finished_step1 = True\n",
    "            solution_vector = vector(GF(2), chain(e1, vector(GF(2), [0]*k)))\n",
    "            e_calculated = H * solution_vector\n",
    "            return e_calculated \n",
    "    \n",
    " \n",
    "    \n",
    "# --- USAGE --- #   \n",
    "# Note: The hamming weight w of the Error e is smaller as the approximated packing radius to decrease the runtime of the algorithms.\n",
    "#       The algorithms also compute the Error e or the Message m when choosing as hamming weight, the packing radius! \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)  \n",
    "\n",
    "primal_e = isd_primal_prange(G, r, n, k, w)\n",
    "if e == primal_e:\n",
    "    print(\"PRIMAL PRANGE WAS SUCCESFUL!\")\n",
    "\n",
    "dual_e = isd_dual_prange(P, s, n, k, w)\n",
    "if e == dual_e:\n",
    "    print(\"DUAL PRANGE WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION OF THE INFORMATION-SET DECODING ALGORITHM OF LEE AND BRICKELL --- #\n",
    "\n",
    "import random \n",
    "from itertools import chain\n",
    "\n",
    "# Input: Dimensions n, k and w(e) = w\n",
    "# Output: Optimal Choice for p to minimize the Runtime Complexity\n",
    "def optimal_runtime_complexity_lee_brickell(n, k, w):\n",
    "    optimal_p = 0     \n",
    "    lowest_complexity = None\n",
    "    for p in range(w+1):\n",
    "        current_complexity = runtime_complexity_lee_brickell(n, k, w, p)\n",
    "        \n",
    "        if lowest_complexity == None or lowest_complexity > current_complexity:\n",
    "                optimal_p = p\n",
    "                lowest_complexity = current_complexity\n",
    "        \n",
    "    return optimal_p\n",
    "        \n",
    "    \n",
    "    \n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n, k, w(e)= w, p the amount of error locations in the information set and the optimal flag            \n",
    "# Output: Error Vector e            \n",
    "def isd_primal_lee_brickell(G, r, n, k, w, p, optimal):\n",
    "    if p > w:\n",
    "        return \"p has to be smaller or equal w!\"\n",
    "\n",
    "    if optimal:\n",
    "        p = optimal_runtime_complexity_lee_brickell() \n",
    "    \n",
    "    success = False\n",
    "    while not success:\n",
    "        I = sorted(random.sample(list(range(n)), k))\n",
    "\n",
    "        G_I = G[:, I]\n",
    "        \n",
    "        if rank(G_I) != k:\n",
    "            continue\n",
    "                \n",
    "        G_I_inverse = G_I.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_I_inverse * G\n",
    "        e_I = r + r_modified * G_new\n",
    "                \n",
    "        # Second Step\n",
    "        if p == 0:\n",
    "            if e_I.hamming_weight() == w:\n",
    "                return e_I\n",
    "        \n",
    "        epsilons = unique_permutations(k, p)\n",
    "        for candidate in epsilons:\n",
    "            e_I_modified = e_I + candidate * G_new\n",
    "            if e_I_modified.hamming_weight() == w:\n",
    "                return e_I_modified   \n",
    "\n",
    "            \n",
    "            \n",
    "# Input: Parity-Check Matrix P, Syndrom s, Dimensions n,k and w(e)= w, outside weight p and the optimal flag\n",
    "#        >> When the optimal flag is set, the optimal choice for p is calculated \n",
    "# Output: Error e\n",
    "def isd_dual_lee_brickell(P, s, n, k, w, p, optimal):\n",
    "    if optimal:\n",
    "        p = optimal_runtime_complexity_lee_brickell()\n",
    "        \n",
    "    finished_step1 = False\n",
    "    \n",
    "    while not finished_step1:\n",
    "        finished_step2 = False\n",
    "        \n",
    "        while not finished_step2:\n",
    "            H = permutation_matrix(n)\n",
    "            PH = P * H\n",
    "            P1 = PH[:,:int(n-k)]\n",
    "            P2 = PH[:, int(n-k):]\n",
    "            is_invertable, P1_inverted = invert_matrix(P1)\n",
    "            \n",
    "            if is_invertable:\n",
    "                finished_step2 = True\n",
    "                \n",
    "        for e2 in unique_permutations(k, p):\n",
    "            e1 = P1_inverted*s + P1_inverted * P2 * e2\n",
    "            if e1.hamming_weight() == w-p:\n",
    "                finished_step1 = True\n",
    "                solution_vector = vector(GF(2), chain(e1, e2))\n",
    "                e_calculated = H * solution_vector\n",
    "                return e_calculated             \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# --- USAGE --- #            \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "p = 1 \n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)  \n",
    "\n",
    "print(\"THE OPTIMAL CHOICE OF THE PARAMETER P IS: P =\", optimal_runtime_complexity_lee_brickell(n, k, w))\n",
    "primal_e = isd_primal_lee_brickell(G, r, n, k, w, p, False)\n",
    "if e == primal_e:\n",
    "    print(\"PRIMAL LEE-BRICKELL WAS SUCCESFUL!\")\n",
    "\n",
    "dual_e = isd_dual_lee_brickell(P, s, n, k, w, p, False)   \n",
    "if e == dual_e:\n",
    "    print(\"DUAL LEE-BRICKELL WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION OF THE INFORMATION-SET DECODING ALGORITHM OF STERN --- #\n",
    "\n",
    "import random\n",
    "import itertools \n",
    "from collections import defaultdict \n",
    "\n",
    "# Input: Dimension n,k and w(e)= w\n",
    "# Output: Optimal Choice for p and l to minimize the Runtime Complexity \n",
    "def optimal_runtime_complexity_isd_stern(n, k, w):\n",
    "    optimal_l, optimal_p = 1, 1\n",
    "    lowest_complexity = None\n",
    "    \n",
    "    for p in range(1, floor(w/2)):\n",
    "        # We have to subtract w-p because the upper factor in the binomial coefficient has to be greater or equal w-p\n",
    "        # Otherwise the Binomial Coefficient cannot be calculated\n",
    "        for l in range(1, n-k):\n",
    "            current_complexity = runtime_complexity_isd_stern(n, k, w, p, l) \n",
    "            if current_complexity == None:\n",
    "                continue \n",
    "                \n",
    "            if lowest_complexity == None or lowest_complexity > current_complexity:\n",
    "                optimal_l, optimal_p = l, p\n",
    "                lowest_complexity = current_complexity\n",
    "    \n",
    "    return optimal_l, optimal_p\n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n and k, the weight of the error w, inside weight p with 2p <= w, Leon's l window and the optimal flag\n",
    "# Output: Error e\n",
    "def isd_stern(G, r, n, k, w, l, p, optimal):\n",
    "    if optimal:\n",
    "        l, p = optimal_runtime_complexity_isd_stern(n, k, w)\n",
    "    \n",
    "    success = False\n",
    "    while not success:\n",
    "        potential_values = list(range(n))\n",
    "        I = sorted(random.sample(potential_values, k))\n",
    "    \n",
    "        if G[:, I].rank() != k:\n",
    "            continue \n",
    "    \n",
    "        r_I = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_I[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G[:, I].inverse() * G\n",
    "        r_modified = r + r_I * G_new \n",
    "        \n",
    "        X = sorted(random.sample(I, floor(k/2)))\n",
    "        Y = sorted(list(filter(lambda a: a not in X, I))) \n",
    "        Z = sorted(random.sample(list(filter(lambda a: a not in I, potential_values)), l))\n",
    "        values_A = []\n",
    "        values_B = []\n",
    "        \n",
    "        counter = 0\n",
    "        for element in I:\n",
    "            if element in X:\n",
    "                values_A.append(counter)\n",
    "            else:\n",
    "                values_B.append(counter)\n",
    "            counter += 1\n",
    "        \n",
    "        phi_A = defaultdict(list)\n",
    "        for current_vector in unique_combinations(values_A, k, p):\n",
    "            value_phi_A = []\n",
    "            result = r_modified + (current_vector * G_new)\n",
    "            for index in Z:\n",
    "                value_phi_A.append(result[index])\n",
    "            phi_A[repr(value_phi_A)].append(current_vector * G_new)\n",
    "        \n",
    "        for candidate in unique_combinations(values_B, k, p):\n",
    "            result = candidate * G_new\n",
    "            value_psi_B = []\n",
    "            for index in Z:\n",
    "                value_psi_B.append(result[index])\n",
    "            \n",
    "            for sum_phi_A in phi_A[repr(value_psi_B)]:\n",
    "                e = r_modified + sum_phi_A + result\n",
    "                if e.hamming_weight() == w:\n",
    "                    return e \n",
    "\n",
    "             \n",
    "      \n",
    "    \n",
    "# --- USAGE --- #                 \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "l, p = optimal_runtime_complexity_isd_stern(n, k, w)\n",
    "\n",
    "print(\"THE OPTIMAL CHOICES OF THE PARAMETER L AND P ARE: L =\", l, \", P =\", p)\n",
    "\n",
    "calculated_e = isd_stern(G, r, n, k, w, l, p, False)\n",
    "if e == calculated_e:\n",
    "    print(\"ISD STERN WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION OF THE INFORMATION-SET DECODING ALGORITHM OF DUMER AND STERN --- #\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input: Dimension n,k and w(e)= w\n",
    "# Output: Optimal Choice for p and l to minimize the Runtime Complexity \n",
    "def optimal_runtime_complexity_dumer_stern(n, k, w):\n",
    "    optimal_l, optimal_p = 1, 2\n",
    "    lowest_complexity = None\n",
    "    \n",
    "    for p in range(2, w+1, 2):\n",
    "        # We have to subtract w-p because the upper factor in the binomial coefficient has to be greater or equal w-p\n",
    "        # Otherwise the Binomial Coefficient cannot be calculated\n",
    "        for l in range(1, n-k-(w-p)+1):\n",
    "            if (l+k)%2 == 0:\n",
    "                current_complexity = runtime_complexity_dumer_stern(n, k, w, p, l) \n",
    "                if current_complexity == None:\n",
    "                    continue \n",
    "                \n",
    "                if lowest_complexity == None or lowest_complexity > current_complexity:\n",
    "                    optimal_l, optimal_p = l, p\n",
    "                    lowest_complexity = current_complexity\n",
    "    \n",
    "    return optimal_l, optimal_p\n",
    "\n",
    "\n",
    "\n",
    "# Input: Parity-Check Matrix P, Syndrom s, Dimension n,k and w(e)= w, outside weight p, Leon's l window and the optimal flag\n",
    "#        >> When the optimal flag is set, the optimal choice for p and l is calculated \n",
    "# Output: Error e   \n",
    "def isd_dumer_stern(P, s, n, k, w, l, p, optimal):\n",
    "    if optimal:\n",
    "        l, p = optimal_runtime_complexity_dumer_stern(n, k, w)\n",
    "      \n",
    "    if p%2 != 0:\n",
    "        return \"p has to be even!\"\n",
    "    \n",
    "    if p < 2:\n",
    "        return \"p has to be at least 2!\"\n",
    "    \n",
    "    if (k+l)%2 != 0:\n",
    "        return \"The sum of k+l has to be even!\"\n",
    "    \n",
    "    success = False\n",
    "    size_e2_e3 = int((k+l)/2)\n",
    "    ones_e2_e3 = int(p/2)\n",
    "    \n",
    "    while not success:\n",
    "        H = permutation_matrix(n)\n",
    "        PH = P * H\n",
    "        is_computable, semi_systematic_P, gaussian_opeations = compute_semi_systematic_form(PH, n, k, l)\n",
    "        \n",
    "        if not is_computable:\n",
    "            continue\n",
    "        \n",
    "        A, B = semi_systematic_P[:int(n-k-l),int(n-k-l):], semi_systematic_P[int(n-k-l):,int(n-k-l):]\n",
    "        A1, A2 = A[:,:int((k+l)/2)], A[:,int((k+l)/2):]\n",
    "        B1, B2 = B[:,:int((k+l)/2)], B[:,int((k+l)/2):]\n",
    "    \n",
    "        Gs = gaussian_opeations * s\n",
    "        s1, s2 = Gs[:int(n-k-l)], Gs[int(n-k-l):]\n",
    "        \n",
    "        L = defaultdict(list)\n",
    "        for e2 in unique_permutations(size_e2_e3, ones_e2_e3):\n",
    "            L[repr(B1 * e2)].append(e2)\n",
    "            \n",
    "        for e3 in unique_permutations(size_e2_e3, ones_e2_e3):\n",
    "            match = s2 + B2*e3\n",
    "            for e2 in L[repr(match)]:\n",
    "                e1 = A1*e2 + A2*e3 + s1\n",
    "                if e1.hamming_weight() == w-p:\n",
    "                    solution_vector = vector(GF(2), chain(e1, e2, e3))\n",
    "                    e = H * solution_vector\n",
    "                    return e \n",
    "\n",
    "\n",
    "                \n",
    "# --- USAGE --- #                \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "l, p = optimal_runtime_complexity_dumer_stern(n, k, w)\n",
    "print(\"THE OPTIMIZED VALUES FOR THE PARAMETERS L AND P ARE: L =\", l, \"AND P =\", p)\n",
    "\n",
    "calculated_e = isd_dumer_stern(P, s, n, k, w, l, p, False)\n",
    "if e == calculated_e: \n",
    "    print(\"ISD DUMER-STERN WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION OF THE INFORMATION-SET DECODING ALGORITHM MMT --- #\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input: Dimension n,k and w(e)= w\n",
    "# Output: Optimal Choice for p and l to minimize the Runtime Complexity \n",
    "def optimal_runtime_complexity_mmt(n, k, w):\n",
    "    optimal_l, optimal_p = 1, 2\n",
    "    lowest_complexity = None\n",
    "    \n",
    "    for p in range(2, w+1, 2):\n",
    "        # We have to subtract w-p because the upper factor in the binomial coefficient has to be greater or equal w-p\n",
    "        # Otherwise the Binomial Coefficient cannot be calculated\n",
    "        for l in range(p+1, n-k-(w-p)+1):\n",
    "            current_complexity = runtime_complexity_mmt(n, k, w, p, l) \n",
    "                \n",
    "            if lowest_complexity == None or current_complexity <= lowest_complexity:\n",
    "                optimal_l, optimal_p  = l, p\n",
    "                lowest_complexity = current_complexity\n",
    "\n",
    "    return optimal_l, optimal_p\n",
    "\n",
    "\n",
    "\n",
    "# Input: Parity-Check Matrix P, Syndrom s, Dimension n,k and w(e)= w, outside weight p, Leon's l window and the optimal flag\n",
    "#        >> When the optimal flag is set, the optimal choice for p and l is calculated \n",
    "# Output: Error e    \n",
    "def isd_mmt(P, s, n, k, w, l, p, optimal):\n",
    "    if optimal:\n",
    "        l, p = optimal_runtime_complexity_mmt(n, k, w)\n",
    "    \n",
    "    if p%2 != 0:\n",
    "        return \"p has to be even!\"\n",
    "    \n",
    "    if p < 2:\n",
    "        return \"p has to be at least 2!\"\n",
    "    \n",
    "    if l < p:\n",
    "        return \"l has to be at least as great as p!\"\n",
    "    \n",
    "    success = False\n",
    "    size_e2_e3 = int(k+l)\n",
    "    ones_e2_e3 = int(p/2)\n",
    "    \n",
    "    while not success: \n",
    "        H = permutation_matrix(n)\n",
    "        PH = P * H\n",
    "        is_computable, semi_systematic_P, gaussian_opeations = compute_semi_systematic_form(PH, n, k, l)\n",
    "        \n",
    "        if not is_computable:\n",
    "            continue\n",
    "        \n",
    "        A, B = semi_systematic_P[:int(n-k-l),int(n-k-l):], semi_systematic_P[int(n-k-l):,int(n-k-l):]\n",
    "        \n",
    "        Gs = gaussian_opeations * s\n",
    "        s1, s2 = Gs[:int(n-k-l)], Gs[int(n-k-l):]\n",
    "          \n",
    "        L1 = defaultdict(list)\n",
    "        for e2 in unique_permutations(size_e2_e3, ones_e2_e3):\n",
    "            value = B * e2\n",
    "            if value[:p] == vector(GF(2), [0]*p):\n",
    "                 L1[repr(value)].append(e2)\n",
    "        \n",
    "        \n",
    "        L2 = defaultdict(list)\n",
    "        for e3 in unique_permutations(size_e2_e3, ones_e2_e3):\n",
    "            value = s2 + B * e3\n",
    "            if value[:p] == vector(GF(2), [0]*p):\n",
    "                 L2[repr(value)].append(e3)\n",
    "               \n",
    "        for Be2, e2 in L1.items():\n",
    "            for e3 in L2[Be2]:\n",
    "                sum_e2e3 = vector(GF(2), vector(GF(2), e2[0]) + vector(GF(2), e3))\n",
    "                e1 = A * sum_e2e3 + s1\n",
    "                if e1.hamming_weight() == w-p:\n",
    "                    solution_vector = vector(GF(2), chain(e1, sum_e2e3))\n",
    "                    e = H * solution_vector\n",
    "                    return e \n",
    "                        \n",
    "\n",
    "                    \n",
    "                    \n",
    "# --- USAGE --- # \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "l = 20\n",
    "p = 2 \n",
    "\n",
    "l_optimized, p_optimized = optimal_runtime_complexity_dumer_stern(n, k, w)\n",
    "print(\"THE OPTIMIZED VALUES FOR THE PARAMETERS L AND P ARE: L =\", l_optimized, \"AND P =\", p_optimized)\n",
    "\n",
    "calculated_e = isd_mmt(P, s, n, k, w, l, p, False)\n",
    "if e == calculated_e:\n",
    "    print(\"ISD MMT WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true
   },
   "outputs": [
   ],
   "source": [
    "# --- CREATING HINT INSTANCES, MENTIONED IN INFORMATION-SET DECODING WITH HINTS --- #\n",
    "# --- THESE HINTS ARE ALL DEFINED OVER GF(2) -- #\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input: Error e ϵ GF(2)^n and amount_of_hints\n",
    "# Output: amount_of_hints random error locations j as Numpy Array\n",
    "# >> Since we are defining the hints over GF(2), we know, that a error entry has the value 1. For this reason, we output in this hint the error location of the error\n",
    "def hint_type_one_two(n, e, amount_of_hints):\n",
    "    if amount_of_hints > e.hamming_weight():\n",
    "        return \"The requested amount of hints is bigger than the amount of error locations in e!\"\n",
    "    \n",
    "    indices = []\n",
    "    for index in range(0,n):\n",
    "        if e[index] == 1:\n",
    "            indices.append(index)\n",
    "            \n",
    "    return np.sort(np.random.permutation(indices)[:amount_of_hints]) \n",
    "\n",
    "\n",
    "\n",
    "# Input: Error e ϵ GF(2)^n and amount_of_hints\n",
    "# Output: amount_of_hints random error-free locations j as Numpy Array\n",
    "def hint_type_three(n, e, amount_of_hints):\n",
    "    if amount_of_hints > n - e.hamming_weight():\n",
    "        return \"The requested amount of hints is bigger than the amount of error-free locations in e!\"\n",
    "    \n",
    "    indices = []\n",
    "    for index in range(0,n):\n",
    "        if e[index] == 0:\n",
    "            indices.append(index)\n",
    "\n",
    "    return np.sort(np.random.permutation(indices)[:amount_of_hints]) \n",
    "\n",
    "\n",
    "\n",
    "# Input: amount_of_hints Measurement Vector v_i ϵ GF(2)^n\\{0} as Array v = [v_1, v_2, v_3, ...] and Error e ϵ GF(2)^n\n",
    "# Output: amount_of_hints sigma_i = e * v_i^t mod 2  as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_type_four(n, v, e, amount_of_hints):\n",
    "    if len(v) != amount_of_hints:\n",
    "        return \"Amount of Hints has to match with the amount of Measurement Vectors in v!\"\n",
    "    \n",
    "    sigmas = []\n",
    "    for index in range(amount_of_hints):\n",
    "        dot_product = e.dot_product(v[index]) \n",
    "        new_entry = [v[index], dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "    \n",
    "    return sigmas\n",
    "\n",
    "\n",
    "\n",
    "# Input: Message m ϵ GF(2)^k and amount_of_hints\n",
    "# Output: amount_of_hints random indices j and the corresponding message entries m_j >> 2D Numpy Array sorted by the Indices \n",
    "def hint_type_five(k, m, amount_of_hints):\n",
    "    if amount_of_hints > k:\n",
    "        return \"The requested amount of hints is bigger than the length of m!\"\n",
    "    \n",
    "    m_indexed = []\n",
    "    for index in range(len(m)):\n",
    "        new_entry = [index, m[index]]\n",
    "        m_indexed.append(new_entry)\n",
    "\n",
    "    hints = np.random.permutation(m_indexed)[:amount_of_hints]\n",
    "    return hints[hints[:,0,].argsort()]\n",
    "\n",
    "\n",
    "\n",
    "# Input: amount_of_hints Measurement Vector v_i ϵ GF(2)^k\\{0} as Array v = [v_1, v_2, v_3, ...] and Message m ϵ GF(2)^k\n",
    "# Output: amount_of_hints sigma_i = m * v_i^t mod 2  as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_type_six(k, v, m, amount_of_hints):\n",
    "    if len(v) != amount_of_hints:\n",
    "        return \"Amount of Hints has to match with the amount of Measurement Vectors in v!\"\n",
    "    \n",
    "    sigmas = []\n",
    "    for index in range(amount_of_hints):\n",
    "        dot_product = m.dot_product(v[index]) \n",
    "        new_entry = [v[index], dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "    \n",
    "    return sigmas\n",
    "\n",
    "\n",
    "\n",
    "# Input: Error Vector e ϵ GF(2)^n, Partitions is a Array, that specifies how {1, 2, ... n} should be divided into amount_partitions partitions\n",
    "#        This Array contains the cardinalities of the corresponding partitions distinct and continous W_i (We only look at blocks of bits here; like in the Paper)\n",
    "#        The Flag equal determines whether {1, 2, ... n} should be divided into partitions of almost equal size \n",
    "# Output: A Array of length amount_partitions, that contains a array that holds the hamming weight t_i = wt_H(e_(W_i)), W_i that holds the W_i's and \n",
    "#         cardinalities_W_i that holds the cardinalities of W_i\n",
    "def hint_hamming_weight(e, n, partitions, amount_partitions, equal):\n",
    "    W_i = []\n",
    "    cardinalities_W_i = []\n",
    "    \n",
    "    if equal:\n",
    "        W = list(np.arange(n))\n",
    "        partitions = np.array_split(W, amount_partitions)\n",
    "        t = []\n",
    "        current_start = 0 \n",
    "        for iterate in partitions:\n",
    "            cardinality_W_i = len(iterate)\n",
    "            W_i.append(np.arange(current_start, current_start + cardinality_W_i))\n",
    "            cardinalities_W_i.append(cardinality_W_i)\n",
    "            e_i = e[current_start:current_start + cardinality_W_i]\n",
    "            current_start += cardinality_W_i\n",
    "            t.append(e_i.hamming_weight())\n",
    "       \n",
    "        return t, W_i, cardinalities_W_i\n",
    "\n",
    "    if sum(partitions) != n:\n",
    "        return \"The specified partitions are incorrect! Please check partitions for misconfigurations.\"\n",
    "    \n",
    "    t = []\n",
    "    current_start = 0\n",
    "    for cardinality_W_i in partitions:\n",
    "        W_i.append(list(np.arange(current_start, current_start + cardinality_W_i)))\n",
    "        cardinalities_W_i.append(cardinality_W_i)\n",
    "        e_i = e[current_start:current_start + cardinality_W_i]\n",
    "        current_start += cardinality_W_i\n",
    "        t.append(e_i.hamming_weight())\n",
    "        \n",
    "    return t, W_i, cardinalities_W_i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- USAGE --- #\n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "amount_of_hints = 3\n",
    "\n",
    "error_locations = hint_type_one_two(n, e, amount_of_hints)\n",
    "print(\"E HAS THE FOLLOWING ERROR LOCATIONS\", error_locations)\n",
    "\n",
    "error_free_locations = hint_type_three(n, e, amount_of_hints)\n",
    "print(\"E HAS THE FOLLOWING ERROR-FREE LOCATIONS\", error_free_locations, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "v_with_P = generate_linearly_independent_v_with_P(P, n, k, amount_of_hints)\n",
    "print(\"FOR THE LINEARLY INDEPENDENT VECTORS WITH THE ROWS OF P V_I:\")\n",
    "for index in range(amount_of_hints):\n",
    "    print(index,\":\", v_with_P[index])  \n",
    "    \n",
    "sigmas_e = hint_type_four(n, v_with_P, e, amount_of_hints)    \n",
    "print(\"\\nWE RECEIVE THE INNER PRODUCTS BETWEEN THE V_I AND E:\")    \n",
    "for index in range(amount_of_hints):\n",
    "    print(index,\":\", sigmas_e[index][1])\n",
    "\n",
    "    \n",
    "\n",
    "entries_m = hint_type_five(k, m, amount_of_hints)    \n",
    "print(\"\\nWE RECEIVE THE MESSAGE ENTRIES M_I AND THEIR CORRESPONDING VALUES:\")\n",
    "for index in range(amount_of_hints):\n",
    "    print(entries_m[index][0], \":\", entries_m[index][1])\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"\\nFOR THE LINEARLY INDEPENDENT VECTORS V_I:\")    \n",
    "v = generate_linearly_independent_v(k, amount_of_hints) \n",
    "for index in range(amount_of_hints):\n",
    "    print(index,\":\", v[index])\n",
    "\n",
    "sigmas_m = hint_type_six(k, v, m, amount_of_hints)   \n",
    "print(\"\\nWE RECEIVE THE INNER PRODUCTS BETWEEN THE V_I AND M:\")    \n",
    "for index in range(amount_of_hints):\n",
    "    print(index,\":\", sigmas_m[index][1])\n",
    "    \n",
    "\n",
    "    \n",
    "amount_partitions = 7    \n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, [], amount_partitions, True) \n",
    "print(\"\\nWE DEFINE THE FOLLOWING BLOCKS EQUAL SIZE:\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", W_i[index])\n",
    "    \n",
    "print(\"\\nTHESE HAVE A CARDINALITY OF\", cardinalities_W_i)    \n",
    "print(\"AND A HAMMING WEIGHT OF\", t)\n",
    "\n",
    "partitions = create_random_partition(amount_partitions, n)\n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, partitions, amount_partitions, False) \n",
    "print(\"\\nWE DEFINE THE FOLLOWING BLOCKS RANDOM SIZE:\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", W_i[index])\n",
    "    \n",
    "print(\"\\nTHESE HAVE A CARDINALITY OF\", cardinalities_W_i)    \n",
    "print(\"AND A HAMMING WEIGHT OF\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- USING HINTS TO TRANSFORM SDP INTO A EASIER SDP INSTANCE --- #\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain \n",
    "\n",
    "# Input: (n,k,w)-SDP defined through the Parity-Check Matrix P, Syndrome s and Error Entries e_j as Array j = [j_1, j_2, ...]\n",
    "# Output: (n-amount_hints, k-X, w-amount_hints)-SDP defined through Parity-Check Matrix P_new and Syndrome s_new\n",
    "# Disclamer: Since the hint types one and two are over GF(2) are identically, the transformations are analog so insted of doing the\n",
    "#            Transformation on the Generator Matrix, we do the analog thing on the Parity Check Matrix.\n",
    "def transformation_hint_type_one_two(P, s, j, amount_of_hints, n, k, w):\n",
    "    # Puncturing of the Dual Code, so the Parity Check Matrix \n",
    "    columns = set(range(n))\n",
    "    new_columns = list(columns.difference(set(j)))\n",
    "    P_modified = P[:, new_columns]\n",
    "\n",
    "    s_new = s \n",
    "    for index in j:\n",
    "        current_column = vector(P[:, index])\n",
    "        s_new = s_new + vector(GF(2), current_column)\n",
    "\n",
    "    # Calculating, the linear independent rows of P_modified >> P_new\n",
    "    P_new = P_modified[0, :]\n",
    "    for index in range(1, n-k):\n",
    "        check = P_new.transpose()\n",
    "        check = check.augment(P_modified[index])\n",
    "        check = check.transpose()\n",
    "        \n",
    "        if rank(check) == index+1:\n",
    "            P_new = check \n",
    "    \n",
    "    n_new = P_new.ncols()\n",
    "    k_new = n_new - P_new.nrows()\n",
    "    w_new = w - amount_of_hints\n",
    "    \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through the Parity-Check Matrix H, Message r = mG + e and a Error-Free Entries e_j as Array j = [j_1, j_2, ...]\n",
    "# Output: (n-amount_hints, k-X, w)-SDP defined through Parity-Check Matrix P_new and Syndrome s_new\n",
    "def transformation_hint_type_three(P, r, j, amount_of_hints, n, k, w):\n",
    "    # Puncturing of the Dual Code, so the Parity Check Matrix \n",
    "    columns = set(range(n))\n",
    "    new_columns = list(columns.difference(set(j)))\n",
    "    P_modified = P[:, new_columns]\n",
    "    s_new = s \n",
    "    \n",
    "    # Calculating, the linear independent rows of P_modified >> P_new\n",
    "    P_new = P_modified[0, :]\n",
    "    for index in range(1, n-k):\n",
    "        check = P_new.transpose()\n",
    "        check = check.augment(P_modified[index])\n",
    "        check = check.transpose()\n",
    "        \n",
    "        if rank(check) == index+1:\n",
    "            P_new = check \n",
    "    \n",
    "    n_new = P_new.ncols()\n",
    "    k_new = n_new - P_new.nrows()\n",
    "    w_new = w\n",
    "    \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through the Parity-Check Matrix P, Syndrome s and amount_of_hints measurement vectors v_i ϵ GF(2)^n\\{0} (v_i is not in the dual code of C) with sigma_i = e * v_i^t mod 2 saved in sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "# Output: (n, k-X, w)-SDP defined through Parity-Check Matrix P_new and Syndrome s_new \n",
    "def transformation_hint_type_four(P, s, sigmas, amount_of_hints, n, k, w):\n",
    "    # Checking whether there are statiscally dependent v_i\n",
    "    useful_v = []\n",
    "    useful_v.append(sigmas[0])\n",
    "    v_matrix = matrix(GF(2), sigmas[0][0])\n",
    "    \n",
    "    for index in range(1, amount_of_hints):\n",
    "        check = v_matrix.transpose()\n",
    "        check = check.augment(sigmas[index][0])\n",
    "        check = check.transpose()\n",
    "        \n",
    "        if rank(check) == index+1:\n",
    "            v_matrix = check\n",
    "            useful_v.append(sigmas[index])\n",
    "    \n",
    "    amount_useful_hints = len(useful_v)\n",
    "    \n",
    "    # Calculating s_new\n",
    "    useful_sigmas = []\n",
    "    for index in range(amount_useful_hints):\n",
    "        useful_sigmas.append(useful_v[index][1])\n",
    "    \n",
    "    s_new = vector(GF(2), chain(s, vector(GF(2), useful_sigmas)))\n",
    "    \n",
    "    # Calculating P_new\n",
    "    P_new = P\n",
    "    for index in range(amount_useful_hints):\n",
    "        P_new = P_new.transpose()\n",
    "        P_new = P_new.augment(useful_v[index][0])\n",
    "        P_new = P_new.transpose()\n",
    "    \n",
    "    \n",
    "    n_new = n\n",
    "    k_new = k - amount_useful_hints\n",
    "    w_new = w \n",
    "    \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "    \n",
    "\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through Generator Matrix G, r = mG+e and amount_of_hints message entries m_j as Array m = [[j_1, m_j_1], [j_2, m_j_2], ...]\n",
    "# Output: (n, k-amount_of_hints, w)-SDP defined through Parity-Check Matrix H' and Syndrome s_new \n",
    "def transformation_hint_type_five(G, r, m, amount_of_hints, n, k, w):\n",
    "    indices = []\n",
    "    messages = []\n",
    "    for index in range(amount_of_hints):\n",
    "        indices.append(m[index][0])\n",
    "        messages.append(m[index][1])\n",
    "    \n",
    "    rows = set(range(k))\n",
    "    new_rows = list(rows.difference(set(indices)))\n",
    "    \n",
    "    G_new = G[new_rows, :]\n",
    "    \n",
    "    n_new = n\n",
    "    k_new = k - amount_of_hints\n",
    "    w_new = w \n",
    "    P_new = from_generator_to_parity_check(G_new)\n",
    "    \n",
    "    # Modifying s; for GF(2) whenever message value is 1 instead of 0 and the Subtraction is Addition in GF(2)\n",
    "    r_new = r \n",
    "    \n",
    "    for current in range(len(indices)):\n",
    "        if messages[current] == 1:\n",
    "            current_row = vector(G[indices[current] ,:])\n",
    "            r_new = r_new + vector(GF(2), current_row)\n",
    "            \n",
    "    s_new = from_errornous_codeword_to_syndrome(P_new, r_new)\n",
    "\n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through Generator Matrix G, r = mG+e and amount_of_hints measurement vectors v_i ϵ GF(2)^k\\{0} with sigma = m * v^t mod 2 saved \n",
    "#        in sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "# Output: (n, k-X, w)-SDP defined through Parity-Check Matrix P_new and Syndrome s_new\n",
    "#         >> Disclamer: In the Paper the return value of the transformation is G' and r', but these are easily convertable into H_new and s_new.\n",
    "#                       So we can use the modified P_new and s_new as Inputs for the ISD Algorithms.\n",
    "def transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w):\n",
    "    # Checking whether there are statiscally dependent v_i\n",
    "    useful_v = []\n",
    "    useful_v.append(sigmas[0])\n",
    "    v_matrix = matrix(GF(2), sigmas[0][0])\n",
    "    \n",
    "    for index in range(1, amount_of_hints):\n",
    "        check = v_matrix.transpose()\n",
    "        check = check.augment(sigmas[index][0])\n",
    "        check = check.transpose()\n",
    "        \n",
    "        if rank(check) == index+1:\n",
    "            v_matrix = check\n",
    "            useful_v.append(sigmas[index])\n",
    "    \n",
    "    amount_useful_hints = len(useful_v)\n",
    "    \n",
    "    k_new = k\n",
    "    for index in range(amount_useful_hints):\n",
    "        current_v, current_sigma = useful_v[index][0], useful_v[index][1]\n",
    "        A = generate_matrix_A(current_v, k, k_new-1)\n",
    "        AG = A * G\n",
    "        G_new = AG[:k_new-1, :]\n",
    "        P_new = from_generator_to_parity_check(G_new)\n",
    "        \n",
    "        r_new = r\n",
    "        if current_sigma == 1:\n",
    "            last_row = vector(AG[k_new-1, :]) \n",
    "            r_new = r_new + vector(GF(2), last_row)\n",
    "        \n",
    "        s_new = from_errornous_codeword_to_syndrome(P_new, r_new)\n",
    "        \n",
    "        n_new = n \n",
    "        k_new = k_new - 1\n",
    "        w_new = w \n",
    "        \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "# --- USAGE --- #\n",
    "n = 237\n",
    "k = 137\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "print(\"THE INITIAL PARAMETERS ARE: n =\", n , \"k =\", k, \"w =\", w)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "amount_of_hints = 3\n",
    "\n",
    "error_locations = hint_type_one_two(n, e, amount_of_hints)\n",
    "P_new, s_new, n_new, k_new, w_new = transformation_hint_type_one_two(P, s, error_locations, amount_of_hints, n, k, w)\n",
    "print(\"NEW PARAMETERS AFTER TRANSFORMATION ONE/TWO ARE: n =\", n_new, \"k =\", k_new, \"w =\", w_new)\n",
    "\n",
    "error_free_locations = hint_type_three(n, e, amount_of_hints)\n",
    "P_new, s_new, n_new, k_new, w_new = transformation_hint_type_three(P, r, error_free_locations, amount_of_hints, n, k, w)\n",
    "print(\"NEW PARAMETERS AFTER TRANSFORMATION THREE ARE: n =\", n_new, \"k =\", k_new, \"w =\", w_new)\n",
    "\n",
    "v_with_P = generate_linearly_independent_v_with_P(P, n, k, amount_of_hints)\n",
    "sigmas_e = hint_type_four(n, v_with_P, e, amount_of_hints)    \n",
    "P_new, s_new, n_new, k_new, w_new = transformation_hint_type_four(P, s, sigmas_e, amount_of_hints, n, k, w)\n",
    "print(\"NEW PARAMETERS AFTER TRANSFORMATION FOUR ARE: n =\", n_new, \"k =\", k_new, \"w =\", w_new)\n",
    "\n",
    "entries_m = hint_type_five(k, m, amount_of_hints) \n",
    "P_new, s_new, n_new, k_new, w_new = transformation_hint_type_five(G, r, entries_m, amount_of_hints, n, k, w)\n",
    "print(\"NEW PARAMETERS AFTER TRANSFORMATION FIVE ARE: n =\", n_new, \"k =\", k_new, \"w =\", w_new)\n",
    "\n",
    "v = generate_linearly_independent_v(k, amount_of_hints)\n",
    "sigmas_m = hint_type_six(k, v, m, amount_of_hints)  \n",
    "P_new, s_new, n_new, k_new, w_new = transformation_hint_type_six(G, r, sigmas_m, amount_of_hints, n, k, w)\n",
    "print(\"NEW PARAMETERS AFTER TRANSFORMATION SIX ARE: n =\", n_new, \"k =\", k_new, \"w =\", w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# --- MAXIMAL AMOUNT OF HITNS PER HINTS FOR UNIQUE DECODING --- #\n",
    "\n",
    "# Input: Dimensions n, k and wt_H(e) = w\n",
    "# Output: The maximal amount of Hints Type One/Two, that can be applied while Decoding is unique \n",
    "def maximal_amount_hint_type_one_two(n, k, w):\n",
    "    max_amount = 0 \n",
    "    for index in range(1, w+1):\n",
    "        if is_unique_decoding(int(n-index), int(k-index), int(w-index)):\n",
    "            max_amount += 1\n",
    "        else:\n",
    "            return max_amount\n",
    "        \n",
    "    return max_amount\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions n, k and wt_H(e) = w\n",
    "# Output: The maximal amount of Hints Type Three, that can be applied while Decoding is unique \n",
    "def maximal_amount_hint_type_three(n, k, w):\n",
    "    for index in range(k-1, 1, -1):\n",
    "        if is_unique_decoding(int(n-index), int(k-index), w):\n",
    "            return index \n",
    "  \n",
    "\n",
    "    \n",
    "# Input: Dimensions n, k and wt_H(e) = w\n",
    "# Output: The maximal amount of Hints Type Four/Five/Six, that can be applied while Decoding is unique     \n",
    "def maximal_amount_hint_type_four_five_six(n, k, w):    \n",
    "    for index in range(k-1, 1, -1):\n",
    "        if is_unique_decoding(n, int(k-index), w):\n",
    "            return index\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# --- USAGE --- #    \n",
    "n = 3488\n",
    "k = 2720 \n",
    "w = 64\n",
    "\n",
    "print(\"WE CAN USE MAXIMAL\", maximal_amount_hint_type_one_two(n, k, w), \"HINTS OF TYPE ONE/TWO.\")\n",
    "\n",
    "maximal_amount_type_three = maximal_amount_hint_type_three(n, k, w)\n",
    "print(\"WE CAN USE MAXIMAL\", maximal_amount_type_three, \"HINTS OF TYPE THREE.\")\n",
    "print(\"WE CAN USE MAXIMAL\", maximal_amount_hint_type_four_five_six(n, k, w), \"HINTS OF TYPE FOUR/FIVE/SIX\")\n",
    "print(\">> WE CAN USE AS MUCH HINTS, AS WE WANT/HAVE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- CHANGES IN RUNTIME OF PRANGE (INFOMRATION-SET DECODING ALGORITHMS) --- #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "\n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when reducing only n\n",
    "def reducing_only_n(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, n-k-w, floor((n-k-w)/100)):\n",
    "        x.append(n-index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(int(n-index), k, w))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('Parameter n')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.title('Reducing only Parameter n')\n",
    "    plt.show()   \n",
    "\n",
    "    \n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when reducing only k    \n",
    "def reducing_only_k(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k, floor(k/100)):\n",
    "        x.append(k-index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(n, int(k-index), w))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[-index] <= 128:\n",
    "            security_level = x[-index]\n",
    "    \n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([0, 500, 1000, 1500, 2000, security_level])\n",
    "    plt.xlabel('Parameter k')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.title('Reducing only Parameter k')\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "\n",
    "    \n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when reducing only w\n",
    "def reducing_only_w(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, w):\n",
    "        x.append(w-index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(n, k, int(w-index)))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[-index] <= 128:\n",
    "            security_level = x[-index]\n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([0, 10, 20, 30, 40, 50, security_level])\n",
    "    plt.xlabel('Parameter t')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.title('Reducing only Parameter t')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "    \n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when reducing n and k equally\n",
    "def reducing_n_k_equally(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k, floor(k/100)):\n",
    "        x.append(index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(int(n-index), int(k-index), w))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[-index] <= 128:\n",
    "            security_level = x[-index]\n",
    "    \n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([0, security_level, 1000, 1500, 2000, 2500])\n",
    "    plt.xlabel('Reducing n and k by')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.title('Reducing Parameters n and k evenly')\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when using the Hint Type One/Two in the Best Case   \n",
    "def reducing_hint_type_one_two(n, k, w):    \n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, w+1):\n",
    "        x.append(index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(int(n-index), int(k-index), int(w-index)))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[index] <= 128:\n",
    "            security_level = x[index]\n",
    "            break \n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([0, security_level, 20, 30, 40, 50, 60])\n",
    "    plt.xlabel('Amount of Hints Type 1/2')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.title('Runtime Complexity SDP-Reduction Hint Type 1/2 to the Base of 2')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "    \n",
    "\n",
    "    \n",
    "# Input: Dimensions n, k and the wt_H(e) = w \n",
    "# Output: Displaying the Distribution of the Packing Radius\n",
    "def distribution_approximated_packing_radius_hint_type_one_two(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, w+1):\n",
    "        x.append(index)\n",
    "        y.append(approximated_packing_radius(n-index, k-index))\n",
    "    \n",
    "    x = x[::-1]\n",
    "    y = y[::-1]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.plot(x,y)\n",
    "    plt.title('Estimated Packing Radius SDP-Reduction Hint Type 1/2')\n",
    "    plt.xlabel('Amount of Hints Type 1/2')\n",
    "    plt.ylabel('Packing Radius')\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "\n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when using the Hint Type Three in the Best Case   \n",
    "def reducing_hint_type_three(n, k, w):    \n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k, floor(k/100)):\n",
    "        x.append(index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(int(n-index), int(k-index), w))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[index] <= 128:\n",
    "            security_level = x[index]\n",
    "            break \n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([0, security_level, 1000, 1500, 2000, 2500])\n",
    "    plt.xlabel('Amount of Hints Type 3')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.title('Runtime Complexity SDP-Reduction Hint Type 3 to the Base of 2')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "    \n",
    "    \n",
    "# Input: Dimensions n, k and the wt_H(e) = w \n",
    "# Output: Displaying the Distribution of the Packing Radius\n",
    "def distribution_approximated_packing_radius_hint_type_three(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k+1, 50):\n",
    "        x.append(index)\n",
    "        y.append(approximated_packing_radius(n-index, k-index))\n",
    "    \n",
    "    x = x[::-1]\n",
    "    y = y[::-1]\n",
    "           \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.plot(x,y)\n",
    "    plt.axhline(y=w, color = 'r', linestyle = '-', label = \"Hamming Weight Error t\")\n",
    "    plt.title('Estimated Packing Radius SDP-Reduction Hint Type 3')\n",
    "    plt.xlabel('Amount of Hints Type 3')\n",
    "    plt.ylabel('Packing Radius')\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: Dimensions n,k and wt_H(e) = w\n",
    "# Output: Resulting Complexity of Prange when using the Hint Type Four/Five/Six       \n",
    "def reducing_hint_type_four_five_six(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k, floor(k/100)):\n",
    "        x.append(index)\n",
    "        y.append(logarithmic_runtime_complexity_prange(n, int(k-index), w))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    security_level = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[index] <= 128:\n",
    "            security_level = x[index]\n",
    "            break \n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([security_level, 500, 1000, 1500, 2000, 2500])\n",
    "    plt.xlabel('Amount of Hints Type 4/5/6')\n",
    "    plt.ylabel('Resulting Runtime of Prange ISD to the base of 2')\n",
    "    plt.axhline(y=128, color = 'black', linestyle = '-', label='Security Level 128')\n",
    "    plt.axvline(x=security_level, color = 'r', linestyle = '--')\n",
    "    plt.title('Runtime Complexity SDP-Reduction Hint Type 4/5/6 to the Base of 2')\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "    \n",
    "\n",
    "    \n",
    "# Input: Dimensions n, k and the wt_H(e) = w \n",
    "# Output: Displaying the Distribution of the Packing Radius\n",
    "def distribution_approximated_packing_radius_hint_type_four_five_six(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, k+1, 50):\n",
    "        x.append(index)\n",
    "        y.append(approximated_packing_radius(n, k-index))\n",
    "    \n",
    "    x = x[::-1]\n",
    "    y = y[::-1]\n",
    "           \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.plot(x,y)\n",
    "    plt.title('Estimated Packing Radius SDP-Reduction Hint Type 4/5/6')\n",
    "    plt.axhline(y = w, color = 'r', linestyle = '-', label='Hamming Weight Error t')\n",
    "    plt.xlabel('Amount of Hints Type 4/5/6')\n",
    "    plt.ylabel('Packing Radius')\n",
    "    plt.legend()\n",
    "    plt.show()        \n",
    "    \n",
    "    \n",
    "    \n",
    "# --- USAGE --- #    \n",
    "\n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "\n",
    "print(\"PRELIMINARY OBSERVATIONS:\")    \n",
    "reducing_only_n(n, k, w)\n",
    "reducing_only_k(n, k, w)\n",
    "reducing_only_w(n, k, w)\n",
    "reducing_n_k_equally(n, k, w)\n",
    "\n",
    "print(\"HINT TYPE ONE/TWO:\")\n",
    "reducing_hint_type_one_two(n, k, w)\n",
    "distribution_approximated_packing_radius_hint_type_one_two(n, k, w)\n",
    "\n",
    "print(\"HINT TYPE THREE:\")\n",
    "reducing_hint_type_three(n, k, w)\n",
    "distribution_approximated_packing_radius_hint_type_three(n, k, w)\n",
    "\n",
    "print(\"HINT TYPE FOUR/FIVE/SIX:\")\n",
    "reducing_hint_type_four_five_six(n, k, w)\n",
    "distribution_approximated_packing_radius_hint_type_four_five_six(n, k, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTATION MODIFIED PRANGE WITH --- # \n",
    "# --- OPTIMIZATION ALGORITHM (GREEDY ALGORITHM) FOR DECIDING THE OPTIMAL X FOR PRANGE'S ISD --- #\n",
    "\n",
    "import random\n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t and the calculated Array x \n",
    "# Output: Success Probability Modified Prange\n",
    "def success_probability_modified_prange(l, W_i, cardinalities_W_i, t, x):\n",
    "    probability = 1\n",
    "    for index in range(l):\n",
    "        probability = probability * (binomial(cardinalities_W_i[index] - x[index], t[index]) / binomial(cardinalities_W_i[index], t[index]))\n",
    "        \n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t and the Dimension k \n",
    "# Output: Array X with cardinalities of Partitions X_i\n",
    "def optimization_modified_isd_prange(l, W_i, cardinalities_W_i, t, k):\n",
    "    x = []\n",
    "    for index in range(l):\n",
    "        x.append(cardinalities_W_i[index] - t[index])\n",
    "        \n",
    "    in_total = sum(x)\n",
    "    \n",
    "    while in_total > k:\n",
    "        current_lowest = 0\n",
    "        current_success_probability = None\n",
    "        \n",
    "        for index in range(l):\n",
    "            modified_x = list(x)\n",
    "            \n",
    "            if modified_x[index] == 0:\n",
    "                continue\n",
    "            \n",
    "            modified_x[index] = modified_x[index] - 1 \n",
    "            # When the probability between all x is the same, the first x_i is decreased\n",
    "            if current_success_probability == None or current_success_probability < success_probability_modified_prange(l, W_i, cardinalities_W_i, t, modified_x):\n",
    "                current_lowest = index \n",
    "                current_success_probability = success_probability_modified_prange(l, W_i, cardinalities_W_i, t, modified_x)\n",
    "                \n",
    "        x[current_lowest] = x[current_lowest] - 1\n",
    "        in_total = in_total - 1 \n",
    "\n",
    "    return x \n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G, r = mG + e, the Amount of Partitions l, t = [wt_H(W_1), ...], W_i = [W_1, ... W_l], cardinalities_W_i = [...], Vector X, Dimensions n,k and w(e) = w\n",
    "# Output: Error e\n",
    "def modified_isd_prange(G, r, l, t, W_i, cardinalities_W_i, X, n, k, w):\n",
    "    success = False\n",
    "    \n",
    "    while not success:\n",
    "        united_X = []\n",
    "        for index in range(l):\n",
    "            if X[index] == 0:\n",
    "                continue\n",
    "                \n",
    "            sample_X = random.sample(list(W_i[index]), X[index])\n",
    "            sample_X.sort()   \n",
    "            for column in sample_X:\n",
    "                united_X.append(column)\n",
    "\n",
    "        G_X = G[:, united_X]\n",
    "        \n",
    "        if rank(G_X) != k:\n",
    "            continue\n",
    "                \n",
    "        G_X_inverse = G_X.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in united_X:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_X_inverse * G\n",
    "        calculated_e = r + r_modified * G_new\n",
    "                \n",
    "        if calculated_e.hamming_weight() == w:\n",
    "            return calculated_e\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "# --- USAGE --- #\n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "amount_partitions = 7    \n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, [], amount_partitions, True) \n",
    "X = optimization_modified_isd_prange(amount_partitions, W_i, cardinalities_W_i, t, k)\n",
    "\n",
    "calculated_e = modified_isd_prange(G, r, amount_partitions, t, W_i, cardinalities_W_i, X, n, k, w)\n",
    "print(\"OUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETER X LOOKS AS FOLLOWS: X =\", X)\n",
    "if e == calculated_e:\n",
    "    print(\">> MODIFIED PRIMAL PRANGE WAS SUCCESSFUL!\")\n",
    "\n",
    "print(\"\\n--- NOW FOR NOT EQUAL SIZED PARTITIONS ---\\n\")    \n",
    "partitions = create_random_partition(amount_partitions, n)\n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, partitions, amount_partitions, False)\n",
    "X = optimization_modified_isd_prange(amount_partitions, W_i, cardinalities_W_i, t, k)\n",
    "\n",
    "calculated_e = modified_isd_prange(G, r, amount_partitions, t, W_i, cardinalities_W_i, X, n, k, w)\n",
    "print(\"OUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETER X LOOKS AS FOLLOWS: X =\", X)\n",
    "if e == calculated_e:\n",
    "    print(\">> MODIFIED PRIMAL PRANGE WAS SUCCESSFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTAION MODIFIED LEE-BRICKELL ISD WITH --- # \n",
    "# --- OPTIMIZATION ALGORITHM (GREEDY ALGORITHM) FOR DECIDING THE OPTIMAL X,Y AND Z FOR LEE-BRICKELL'S ISD --- #\n",
    "\n",
    "import random \n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t, \n",
    "#        the calculated Array x and the amount of errors outside the information set notated with p\n",
    "# Output: Success Probability Modified Lee-Brickell and None when the Probability is not defined  \n",
    "def success_probability_modified_lee_brickell(l, W_i, cardinalities_W_i, t, x, p):\n",
    "    if p == 0:\n",
    "        return success_probability_modified_prange(l, W_i, cardinalities_W_i, t, x)\n",
    "     \n",
    "    possible_a = all_possible_sums(p, l)\n",
    "    probability = 0\n",
    "    for a in possible_a:\n",
    "        current_probability = 1\n",
    "        for index in range(l):\n",
    "            if x[index] == 0:\n",
    "                return\n",
    "            \n",
    "            numerator = binomial(x[index], int(a[index])) * binomial(cardinalities_W_i[index] - x[index], t[index] - int(a[index]))\n",
    "            denominator = binomial(cardinalities_W_i[index], t[index])\n",
    "            \n",
    "            if numerator != 0 and denominator != 0:\n",
    "                current_probability = current_probability * (numerator / denominator) \n",
    "           \n",
    "        probability += current_probability\n",
    "   \n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t,\n",
    "#        the Dimension k and the amount of errors outside the information set notated with p \n",
    "# Output: Array X with cardinalities of Partitions X_i\n",
    "def optimization_modified_isd_lee_brickell(l, W_i, cardinalities_W_i, t, k, p):\n",
    "    x = []\n",
    "    for index in range(l):\n",
    "        x.append(cardinalities_W_i[index] - t[index])\n",
    "        \n",
    "    in_total = sum(x)\n",
    "    \n",
    "    while in_total > k:\n",
    "        current_lowest = 0\n",
    "        current_success_probability = None\n",
    "        \n",
    "        for index in range(l):\n",
    "            modified_x = list(x)\n",
    "            \n",
    "            if modified_x[index] == 0:\n",
    "                continue\n",
    "            \n",
    "            modified_x[index] = modified_x[index] - 1 \n",
    "            # When the probability between all x is the same, the first x_i is decreased\n",
    "            success = success_probability_modified_lee_brickell(l, W_i, cardinalities_W_i, t, modified_x, p)\n",
    "            if success == None:\n",
    "                continue\n",
    "            \n",
    "            if current_success_probability == None or current_success_probability < success:\n",
    "                current_lowest = index \n",
    "                current_success_probability = success\n",
    "\n",
    "                \n",
    "        if x[current_lowest] != 0:\n",
    "            x[current_lowest] = x[current_lowest] - 1\n",
    "        # Edge Case: All x_i have the same probability!\n",
    "        else:\n",
    "            while True:\n",
    "                index = random.randint(0,l-1)\n",
    "                if index != current_lowest and x[index] != 0:\n",
    "                    x[index] = x[index] - 1\n",
    "                    break\n",
    "            \n",
    "        in_total = in_total - 1 \n",
    "\n",
    "    return x \n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G, r = mG + e, the Amount of Partitions l, t = [wt_H(W_1), ...], W_i = [W_1, ... W_l], cardinalities_W_i = [...], Vector X, Dimensions n,k \n",
    "#        w(e) = w and p the amount of errors outside the information set \n",
    "# Output: Error e\n",
    "def modified_isd_lee_brickell(G, r, l, t, W_i, cardinalities_W_i, X, n, k, w, p, optimal):\n",
    "    if p > w:\n",
    "        return \"p has to be smaller or equal w!\"\n",
    "\n",
    "    if optimal:\n",
    "        p = 0 \n",
    "        \n",
    "    success = False\n",
    "    while not success:\n",
    "        # Pre-Computations \n",
    "        united_X = []\n",
    "        for index in range(l):\n",
    "            if X[index] == 0:\n",
    "                continue\n",
    "                \n",
    "            sample_X = random.sample(list(W_i[index]), X[index])\n",
    "            sample_X.sort()   \n",
    "            for column in sample_X:\n",
    "                united_X.append(column)\n",
    "\n",
    "        G_X = G[:, united_X]\n",
    "        \n",
    "        if rank(G_X) != k:\n",
    "            continue\n",
    "                \n",
    "        G_X_inverse = G_X.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in united_X:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_X_inverse * G\n",
    "        e_I = r + r_modified * G_new\n",
    "                \n",
    "        # Second Step\n",
    "        if p == 0:\n",
    "            if e_I.hamming_weight() == w:\n",
    "                return e_I\n",
    "        \n",
    "        epsilons = unique_permutations(k, p)\n",
    "        for candidate in epsilons:\n",
    "            e_I_modified = e_I + candidate * G_new\n",
    "            if e_I_modified.hamming_weight() == w:\n",
    "                return e_I_modified\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "# --- USAGE --- #\n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "  \n",
    "p = 1\n",
    "amount_partitions = 7 \n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, [], amount_partitions, True) \n",
    "X = optimization_modified_isd_lee_brickell(amount_partitions, W_i, cardinalities_W_i, t, k, p)\n",
    "\n",
    "print(\"THE OPTIMAL VALUE FOR THE PARAMETER P IS 0!\")\n",
    "print(\"DISCLAMER: CALCULATING THE OPTIMIZED X IS VERY COSTLY!\\n\")\n",
    "\n",
    "print(\"OUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETER X LOOKS AS FOLLOWS: X =\", X)\n",
    "calculated_e = modified_isd_lee_brickell(G, r, amount_partitions, t, W_i, cardinalities_W_i, X, n, k, w, p, False)\n",
    "if e == calculated_e:\n",
    "    print(\">> MODIFIED PRIMAL LEE-BRICKELL WAS SUCCESFUL!\")\n",
    "\n",
    "    \n",
    "print(\"\\n--- NOW FOR NOT EQUAL SIZED PARTITIONS ---\\n\")    \n",
    "partitions = create_random_partition(amount_partitions, n)\n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, partitions, amount_partitions, False)\n",
    "X = optimization_modified_isd_lee_brickell(amount_partitions, W_i, cardinalities_W_i, t, k, p)\n",
    "\n",
    "calculated_e = modified_isd_lee_brickell(G, r, amount_partitions, t, W_i, cardinalities_W_i, X, n, k, w, p, False)\n",
    "\n",
    "print(\"OUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETER X LOOKS AS FOLLOWS: X =\", X)\n",
    "if e == calculated_e:\n",
    "    print(\">> MODIFIED PRIMAL LEE-BRICKELL WAS SUCCESFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- IMPLEMENTAION MODIFIED DUMER-STERN ISD WITH --- # \n",
    "# --- OPTIMIZATION ALGORITHM (GREEDY ALGORITHM) FOR DECIDING THE OPTIMAL X,Y AND Z FOR DUMER-STERN'S ISD --- #\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t, \n",
    "#        the calculated Array x,y and z and half of the amount of errors outside the information set notated with p\n",
    "# Output: Success Probability Modified Stern and None when the Probability is not defined\n",
    "def success_probability_modified_stern(l, W_i, cardinalities_W_i, t, x, y, z, p):\n",
    "    if p == 0:\n",
    "        return success_probability_modified_prange(l, W_i, cardinalities_W_i, t, x)\n",
    "    \n",
    "    possible_a = all_possible_sums(p, l)\n",
    "    possible_b = all_possible_sums(p, l)\n",
    "    probability = 0\n",
    "    for a in possible_a:\n",
    "        current_probability = 1\n",
    "        for b in possible_b:\n",
    "            for index in range(l):\n",
    "                numerator = binomial(x[index], int(a[index])) * binomial(y[index], int(b[index])) \n",
    "                numerator *= binomial(cardinalities_W_i[index] - x[index] - y[index] - z[index] ,t[index] - int(a[index]) - int(b[index]))\n",
    "                denominator = binomial(cardinalities_W_i[index], t[index])\n",
    "                if numerator != 0 and denominator != 0:\n",
    "                    current_probability = current_probability * (numerator / denominator) \n",
    "                \n",
    "            probability += current_probability\n",
    "\n",
    "    return probability\n",
    "\n",
    "\n",
    "\n",
    "# Input: The Amount of Partitions l, a Partition Array W_i, the cardinalities of the Partitions as Array cardinalities, the Hamming Weight of the blocks as Array t,\n",
    "#        the Dimension k, the amount of errors outside the information set notated with p and the switch flag to determine whether x is set through the\n",
    "#        Prange Optimization or through the Lee-Brickell Optimization\n",
    "# Output: Array X, Y and Z with cardinalities of Partitions X_i, Y_i and Z_i\n",
    "def optimization_modified_isd_stern(l, W_i, cardinalities_W_i, t, k, p, v, switch):\n",
    "    if v > n - k:\n",
    "        return \"v has to be smaller than n-k!\"\n",
    "    \n",
    "    if switch:\n",
    "        x_tilde = optimization_modified_isd_lee_brickell(l, W_i, cardinalities_W_i, t, k, int(2*p))\n",
    "    else:\n",
    "        x_tilde = optimization_modified_isd_prange(l, W_i, cardinalities_W_i, t, k)\n",
    "        \n",
    "    x = []\n",
    "    y = []\n",
    "    round_x_up = True\n",
    "    \n",
    "    for element in x_tilde:\n",
    "        if element % 2 == 0:\n",
    "            x.append(int(element/2))\n",
    "            y.append(int(element/2))\n",
    "        else:\n",
    "            if round_x_up:\n",
    "                x.append(ceil(element/2))\n",
    "                y.append(floor(element/2))\n",
    "                round_x_up = False\n",
    "            else:\n",
    "                x.append(floor(element/2))\n",
    "                y.append(ceil(element/2))\n",
    "                round_x_up = True\n",
    "                \n",
    "    z = [0] * l\n",
    "    while sum(z) < v:\n",
    "        current_highest = 0\n",
    "        current_success_probability = None\n",
    "        \n",
    "        for index in range(l):\n",
    "            if cardinalities_W_i[index] == x[index] + y[index] + z[index]:\n",
    "                continue\n",
    "            \n",
    "            modified_z = list(z)\n",
    "            modified_z[index] += 1 \n",
    "            success = success_probability_modified_stern(l, W_i, cardinalities_W_i, t, x, y, z, p)\n",
    "            if success == None:\n",
    "                continue\n",
    "            \n",
    "            if current_success_probability == None or current_success_probability < success:\n",
    "                current_highest = index\n",
    "                current_success_probability = success\n",
    "                \n",
    "        z[current_highest] += 1\n",
    "        \n",
    "    return x, y, z\n",
    "            \n",
    "    \n",
    "    \n",
    "# Input: Generator Matrix G, r = mG + e, the Amount of Partitions l, t = [wt_H(W_1), ...], W_i = [W_1, ... W_l], cardinalities_W_i = [...], Vector X, Y and Z, Dimensions n,k \n",
    "#        w(e) = w, p the amount of errors inside the information set, v and the optimal flag for optimized v and p \n",
    "# Output: Error e\n",
    "def modified_isd_stern(G, r, l, t, W_i, cardinalities_W_i, X, Y, Z, n, k, w, p, v, optimal):\n",
    "    if optimal:\n",
    "        v, p = optimal_runtime_complexity_isd_stern(n, k, w)\n",
    "    \n",
    "    success = False\n",
    "    while not success:\n",
    "        # Precomputations, compute X, Y and Z \n",
    "        united_X = []\n",
    "        united_Y = []\n",
    "        united_Z = []\n",
    "        for index in range(l):\n",
    "            W_without_X = list(W_i[index])\n",
    "            W_without_X_Y = list(W_i[index])\n",
    "            \n",
    "            if X[index] != 0:\n",
    "                sample_X = sorted(random.sample(list(W_i[index]), X[index]))\n",
    "                sample_X.sort()   \n",
    "                for column in sample_X:\n",
    "                    united_X.append(column)\n",
    "                    \n",
    "                W_without_X = list(filter(lambda a: a not in sample_X, W_i[index]))\n",
    "                    \n",
    "            if Y[index] != 0:\n",
    "                sample_Y = sorted(random.sample(W_without_X, Y[index]))\n",
    "                for column in sample_Y:\n",
    "                    united_Y.append(column)\n",
    "                    \n",
    "                W_without_X_Y = list(filter(lambda a: a not in sample_Y, W_without_X))\n",
    "                \n",
    "            if Z[index] != 0:\n",
    "                sample_Z = sorted(random.sample(W_without_X_Y, Z[index]))\n",
    "                for column in sample_Z:\n",
    "                    united_Z.append(column)\n",
    "        \n",
    "        I = sorted(united_X + united_Y)\n",
    "    \n",
    "        if G[:, I].rank() != k:\n",
    "            continue \n",
    "            \n",
    "        r_I = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_I[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G[:, I].inverse() * G\n",
    "        r_modified = r + r_I * G_new    \n",
    "        \n",
    "        values_A = []\n",
    "        values_B = []\n",
    "        \n",
    "        counter = 0\n",
    "        for element in I:\n",
    "            if element in united_X:\n",
    "                values_A.append(counter)\n",
    "            else:\n",
    "                values_B.append(counter)\n",
    "            counter += 1\n",
    "        \n",
    "        phi_A = defaultdict(list)\n",
    "        for current_vector in unique_combinations(values_A, k, p):\n",
    "            value_phi_A = []\n",
    "            result = r_modified + current_vector * G_new\n",
    "            for index in Z:\n",
    "                value_phi_A.append(result[index])\n",
    "            phi_A[repr(value_phi_A)].append(current_vector * G_new)\n",
    "        \n",
    "        for candidate in unique_combinations(values_B, k, p):\n",
    "            result = candidate * G_new\n",
    "            value_psi_B = []\n",
    "            for index in Z:\n",
    "                value_psi_B.append(result[index])\n",
    "            \n",
    "            for sum_phi_A in phi_A[repr(value_psi_B)]:\n",
    "                e = r_modified + sum_phi_A + result\n",
    "                if e.hamming_weight() == w:\n",
    "                    return e \n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "# --- USAGE --- #                 \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "amount_of_partitions = 7\n",
    "v, p = optimal_runtime_complexity_isd_stern(n, k, w)\n",
    "print(\"THE OPTIMAL VALUES FOR THE PARAMETERS V AND P ARE: V =\", v, \"and P =\", p)\n",
    "print(\"DISCLAMER: WHEN USING THE LEE-BRICKELL OPTIMIZATION CALCULATING THE OPTIMIZED X, Y, Z IS VERY COSTLY!\\n\")\n",
    "\n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, [], amount_of_partitions, True)\n",
    "X, Y, Z = optimization_modified_isd_stern(amount_of_partitions, W_i, cardinalities_W_i, t, k, p, v, True)\n",
    "X_p, Y_p, Z_p = optimization_modified_isd_stern(amount_of_partitions, W_i, cardinalities_W_i, t, k, p, v, False)\n",
    "\n",
    "calculated_e = modified_isd_stern(G, r, amount_of_partitions, t, W_i, cardinalities_W_i, X, Y, Z, n, k, w, p, v, False)    \n",
    "print(\"\\nOUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETERS LOOK AS FOLLOWS (LEE-BRICKELL): \\nX =\", X, \"\\nY =\", Y, \"\\nZ =\", Z, \"\\nv =\", v, \"\\np =\", p)\n",
    "print(\"\\nTHE PARAMETERS LOOK AS FOLLOWS (PRANGE): \\nX =\", X_p, \"\\nY =\", Y_p, \"\\nZ =\", Z_p)\n",
    "\n",
    "if calculated_e == e:\n",
    "    print(\">> THE MODIFIED ISD STERN WAS SUCCESFUL!\")   \n",
    "    \n",
    "print(\"\\n--- NOW FOR NOT EQUAL SIZED PARTITIONS ---\\n\")    \n",
    "partitions = create_random_partition(amount_of_partitions, n)\n",
    "t, W_i, cardinalities_W_i = hint_hamming_weight(e, n, partitions, amount_of_partitions, False)\n",
    "X, Y, Z = optimization_modified_isd_stern(amount_of_partitions, W_i, cardinalities_W_i, t, k, p, v, False)\n",
    "X_lee, Y_lee, Z_lee = optimization_modified_isd_stern(amount_of_partitions, W_i, cardinalities_W_i, t, k, p, v, True)\n",
    "\n",
    "calculated_e = modified_isd_stern(G, r, amount_of_partitions, t, W_i, cardinalities_W_i, X, Y, Z, n, k, w, p, v, False)    \n",
    "print(\"OUR PARTITIONS HAVE THE CARDINALITIES OF: cardinalities_W_i =\", cardinalities_W_i)\n",
    "print(\"THE PARTITIONS HAVE FOLLOWING HAMMING WEIGHT: t =\", t)\n",
    "print(\"THE PARAMETERS LOOK AS FOLLOWS (PRANGE): \\nX =\", X, \"\\nY =\", Y, \"\\nZ =\", Z)\n",
    "\n",
    "if calculated_e == e:\n",
    "    print(\">> THE MODIFIED ISD STERN WAS SUCCESFUL!\")  \n",
    "    \n",
    "print(\"\\nIF Z IS ZERO EVERYWHERE THE PROBABILITY IS THE SAME FOR ALL FREE LOCATIONS. AFTER THE INCREMENT THE INCREMENTED SPOT INCREASES THE PROBABILITY THE MOST!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- OPTIMIZATION TRANSFORMATION HINT TYPE SIX FOR PROCESSING MULTIPLE HINTS AT ONCE --- #\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through Generator Matrix G, r = mG+e and amount_of_hints measurement vectors v_i ϵ GF(2)^k\\{0}  \n",
    "#         with sigma = m * v^t mod 2 saved in sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "# Output: (n, k-X, w)-SDP defined through Parity-Check Matrix P_new and Syndrome s_new\n",
    "def optimization_transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w):\n",
    "    if amount_of_hints == 1:\n",
    "        return transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w)\n",
    "    \n",
    "    hint_matrix = matrix(GF(2), k+1, amount_of_hints)\n",
    "    \n",
    "    for column in range(amount_of_hints):\n",
    "        new_entry = list(sigmas[column][0])\n",
    "        new_entry.append(sigmas[column][1])\n",
    "        new_entry = vector(GF(2), new_entry)\n",
    "        hint_matrix[:, column] = new_entry\n",
    "        \n",
    "    if rank(hint_matrix) != amount_of_hints:\n",
    "        return \"The Hint Vectors have to be linearly independent!\"\n",
    "    \n",
    "    V = hint_matrix[:len(sigmas[0][0]), :] \n",
    "    Sigma = hint_matrix[len(sigmas[0][0]):, :]\n",
    "    \n",
    "    V_1 = V[:amount_of_hints, :]\n",
    "    \n",
    "    if rank(V_1) != amount_of_hints:\n",
    "        return \"V_1 is not invertable!\"\n",
    "    \n",
    "    left_matrix_part = hint_matrix * V_1.inverse()\n",
    "    \n",
    "    right_matrix_part = matrix(GF(2), k+1, n)\n",
    "    \n",
    "    for column in range(n):\n",
    "        new_entry = []\n",
    "        for row in range(G.nrows()):\n",
    "            new_entry.append(G[row, column])\n",
    "            \n",
    "        new_entry.append(r[column])\n",
    "        new_entry = vector(GF(2), new_entry)\n",
    "        right_matrix_part[:, column] = new_entry\n",
    "        \n",
    "    resulting_matrix = left_matrix_part.augment(right_matrix_part)\n",
    "    \n",
    "    for column in range(amount_of_hints, resulting_matrix.ncols()):\n",
    "        for row in range(0, amount_of_hints):\n",
    "            if resulting_matrix[row, column] == 1:\n",
    "                resulting_matrix[:, column] += resulting_matrix[:, row]\n",
    "        \n",
    "    right_part = resulting_matrix[:, amount_of_hints:]\n",
    "    G_new = right_part[amount_of_hints:k , :]\n",
    "    last_row = right_part[k: , :]\n",
    "    r_new = vector(GF(2), n)\n",
    "    for index in range(n):\n",
    "        r_new[index] = last_row[0][index]\n",
    "    \n",
    "    k_new = G_new.nrows()\n",
    "    n_new = G_new.ncols()\n",
    "    w_new = w \n",
    "    \n",
    "    P_new = from_generator_to_parity_check(G_new)\n",
    "    s_new = P_new * r_new \n",
    "    \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- USAGE --- # \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "print(\"FOR SIMPLICITY REASONS THIS TRANSFORMATION ONLY WORKS WHEN THE MATRIX V_1 IS INVERTABLE!\")\n",
    "amount_of_hints = 3\n",
    "v = generate_linearly_independent_v(k, amount_of_hints) \n",
    "sigmas = hint_type_six(k, v, m, amount_of_hints) \n",
    "    \n",
    "try:\n",
    "    P_new, s_new, n_new, k_new, w_new = optimization_transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w)\n",
    "    print(\"\\nTHE TEST INSTANCE HAS PARAMETERS: N =\", n, \"K =\", k, \"w =\", w)\n",
    "    print(\"AFTER THE TRANSFORMATION NEW PARAMETERS ARE: N =\", n_new, \"K =\", k_new, \"w =\", w_new) \n",
    "except:\n",
    "    print(\"PLEASE TRY IT AGAIN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- ATTEMPT FOR A STRONGER TRANSFORMATION THROUGH MULTIPLE HINTS OF HINT TYPE SIX --- # \n",
    "# NOTE: This Transformation is also only possible if in the smaller instance decoding with the weight of w is unique.\n",
    "#       >> Otherwise if the Transformation is successful and the Decoding is not unique, we have to use List Decoding!\n",
    "\n",
    "import numpy as np \n",
    "import copy\n",
    "import random  \n",
    "import inspect\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Input: (n,k,w)-SDP defined through Generator Matrix G, r = mG+e and amount_of_hints measurement vectors v_i ϵ GF(2)^k\\{0}  \n",
    "#         with sigma = m * v^t mod 2 saved in sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "# Output: (n-X, k-Y, w)-SDP defined through Parity-Check Matrix P_new, Syndrome s_new and a Array sigmas, that contains error-free and error locations\n",
    "def attempt_optimization_transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w):\n",
    "    injection_points = random.sample(list(np.arange(n)), amount_of_hints) \n",
    "    G_new = copy.copy(G) \n",
    "    r_new = r\n",
    "    w_new = w\n",
    "    new_sigmas = []\n",
    "    \n",
    "    # Calculating the modified SDP-Instance with known error and error-free location\n",
    "    # Here is the assumption, that the error is always error-free in the injection points \n",
    "    # >> The gained weight \n",
    "    counter = 0\n",
    "    for index in injection_points:\n",
    "        G_new[:, index] = sigmas[counter][0]\n",
    "        r_new[index] = 0\n",
    "        new_sigmas.append([index, sigmas[counter][1]])\n",
    "        if sigmas[counter][1] == 1:\n",
    "            w_new += 1 \n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    P_new = from_generator_to_parity_check(G_new)\n",
    "    s_new = P_new * r_new \n",
    "    \n",
    "    # Using the error and error-free locations to create a much smaller SDP-Instance\n",
    "    for index in range(len(new_sigmas)):\n",
    "        if new_sigmas[index][1] == 0:\n",
    "            P_new, s_new, n_new, k_new, w_new = transformation_hint_type_three(P_new, s_new, [new_sigmas[index][0]], 1, P_new.ncols(), P_new.ncols() -P_new.nrows(), w_new)\n",
    "        if new_sigmas[index][1] == 1:\n",
    "            P_new, s_new, n_new, k_new, w_new = transformation_hint_type_one_two(P_new, s_new, [new_sigmas[index][0]], 1, P_new.ncols(), P_new.ncols() - P_new.nrows(), w_new)\n",
    "        \n",
    "    return P_new, s_new, n_new, k_new, w_new\n",
    "\n",
    "\n",
    "\n",
    "# Input: n, w = wt_H(e) and the amount_of_hints\n",
    "# Output: Success Probability of the Optimization Attempt\n",
    "def success_probability_attempt_hint_type_six(n, w, amount_of_hints):\n",
    "    success_probability = 1 \n",
    "    for index in range(0, amount_of_hints):\n",
    "        success_probability = success_probability * ((n-w-index) / n) \n",
    "        \n",
    "    return float(success_probability)\n",
    "\n",
    "\n",
    "\n",
    "# Input: n, w = wt_H(e) and the amount_of_hints\n",
    "# Output: Displays the Success Probability of the Transformation when using x Hints, where 1 <= x <= amount_of_hints\n",
    "def distribution_success_probability_attempt(n, w, amount_of_hints):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(1, amount_of_hints+1):\n",
    "        x.append(index)\n",
    "        y.append(success_probability_attempt_hint_type_six(n, w, index))\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Success Probability Attempt Stronger Transformation Hint Type Six')\n",
    "    plt.ylabel('Success Probability')\n",
    "    plt.xlabel('Amount of Hints')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n,k and w(e) <= w \n",
    "# Output: Error e\n",
    "def modified_isd_primal_prange(G, r, n, k, w):\n",
    "    success = False\n",
    "    \n",
    "    while not success:\n",
    "        I = sorted(random.sample(list(range(n)), k))\n",
    "    \n",
    "        if G[:, I].rank() != k:\n",
    "            continue \n",
    "\n",
    "        G_I = G[:, I]\n",
    "        G_I_inverse = G_I.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_I_inverse * G\n",
    "        calculated_e = r + r_modified * G_new\n",
    "                \n",
    "        if calculated_e.hamming_weight() <= w:\n",
    "            return r_modified * G_I_inverse   \n",
    "    \n",
    "    \n",
    "    \n",
    "# Input: Parameters n, k and w(e) = w\n",
    "# Output: Displaying a Graph, that shows how many inner products have to be 1 to be more efficient    \n",
    "def success_stronger_transformation(n, k, w):\n",
    "    x = []\n",
    "    y_normal = []\n",
    "    y_worst_optimized = []\n",
    "    \n",
    "    for index in range(0, k, int(2720/100)):\n",
    "        x.append(index)\n",
    "        y_normal.append(logarithmic_runtime_complexity_prange(n, k-index, w))\n",
    "        y_worst_optimized.append(logarithmic_runtime_complexity_prange(n-index, k-index, w))\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y_normal = np.array(y_normal)\n",
    "    y_worst_optimized = np.array(y_worst_optimized)\n",
    "        \n",
    "    plt.plot(x, y_normal, 'b', label=\"Normal Transformation\")\n",
    "    plt.plot(x, y_worst_optimized, 'r', label=\"Worst Case Optimization\")\n",
    "    plt.ylabel('Resulting Runtime Prange ISD to the base of 2')\n",
    "    plt.xlabel('Amount of Hints Type 6')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    y_amount_ones = []\n",
    "    y_success_probability = []\n",
    "    \n",
    "    \n",
    "    for index in range(0, k, int(2720/100)):\n",
    "        to_beat = logarithmic_runtime_complexity_prange(n, k-index, w)\n",
    "        for amount in range(w):\n",
    "            current = logarithmic_runtime_complexity_prange(n-index, k-index, w-amount)\n",
    "            if current <= to_beat:\n",
    "                y_amount_ones.append(amount)\n",
    "                y_success_probability.append(float(1 - cumulative_hypergeometric_distribution(n, w, index, amount)))\n",
    "                break\n",
    "    \n",
    "    y_amount_ones = np.array(y_amount_ones)\n",
    "    plt.plot(x, y_amount_ones)            \n",
    "    plt.ylabel(\"Minimal Amount of Deleted Ones\")\n",
    "    plt.xlabel(\"Amount of Hints Type 6\")\n",
    "    plt.show()\n",
    "    \n",
    "    y_success_probability = np.array(y_success_probability)\n",
    "    plt.plot(x, y_success_probability)\n",
    "    plt.ylabel(\"Success Probabilty for Stronger Optimization\")\n",
    "    plt.xlabel(\"Amount of Hints Type 6\")\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# --- USAGE --- # \n",
    "n = 3488\n",
    "k = 2720\n",
    "amount_of_hints = 500\n",
    "\n",
    "print(\"THE TRANSFORMATION ASSUMES, THAT THE DELETED POSITIONS OF E ARE ALL ERROR-FREE, SO THE INJECTION POINTS HAS TO BE ERROR-FREE.\")\n",
    "print(\"FURTHERMORE THIS TRANSFORMATION ASSUMES, THAT DECODING IS STILL UNIQUE IN THE NEW INSTANCE FOR THE WEIGHT W!\\n\")\n",
    "print(\"SUCCESS RATE OF THE ATTEMPT IN A MCELIECE SETTING:\")\n",
    "distribution_success_probability_attempt(n, w, amount_of_hints)\n",
    "\n",
    "n = 237\n",
    "k = 124\n",
    "amount_of_hints = 3 \n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "\n",
    "print(\"\\nSUCCESS RATE OF THE ATTEMPT IN THE CURRENT SETTING:\")\n",
    "distribution_success_probability_attempt(n, w, amount_of_hints)\n",
    "\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "v = generate_linearly_independent_v(k, amount_of_hints) \n",
    "sigmas = hint_type_six(k, v, m, amount_of_hints) \n",
    "P_new, s_new, n_new, k_new, w_new = attempt_optimization_transformation_hint_type_six(G, r, sigmas, amount_of_hints, n, k, w)\n",
    "print(\"\\nTHE TEST INSTANCE HAS PARAMETERS: N =\", n, \"K =\", k, \"w =\", w)\n",
    "print(\"AFTER THE TRANSFORMATION NEW PARAMETERS ARE: N =\", n_new, \"K =\", k_new, \"w =\", w_new)\n",
    "\n",
    "print(\"BY ASSUMING THAT DECODING IS STILL UNIQUE FOR A ERROR WEIGHT OF W WE CAN OPTIMIZE THE TRANSFORMATION BY INTRODUCING A MODIFIED PRANGE: \\n\")\n",
    "\n",
    "code, line_no = inspect.getsourcelines(modified_isd_primal_prange)\n",
    "print(''.join(code))\n",
    "\n",
    "print(\"\\nTHIS ALGORITHM IS STILL CORRECT SINCE DECODING IS UNIQUE FOR ERROR VECTORS OF HAMMING WEIGHT W AND OUR ERROR IS SMALLER OR EQUAL THAN W\")\n",
    "print(\">> NOW THE TRANSFORMATION WORKS EVERYTIME, SINCE WE ARE ALLOWING THAT THE DELETED POSITIONS OF E ALSO CAN CONTAIN ERROR LOCATIONS! :D\")\n",
    "\n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "\n",
    "print(\"\\nTRANSFORMATION IN A MCELIECE SETTING:\")\n",
    "success_stronger_transformation(n, k, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- CREATING VARIANTS OF THE HINT INSTANCES, MENTIONED IN INFORMATION-SET DECODING WITH HINTS --- # \n",
    "\n",
    "# Input: measurement vector v ϵ GF(2)^k\\{0}, Message m ϵ GF(2)^k and Partitions Array, that v divides into v = v_1|v_2|....|v_x by giving the cardinalities of v_i     \n",
    "# Output: amount of partitions sigma_i = m_i * v_i^t as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_variant_blocks_v_m(k, v, m, partitions):\n",
    "    if sum(partitions) != k:\n",
    "        return \"v and m cannot be divided into the stated partitions!\"\n",
    "    \n",
    "    sigmas = []\n",
    "    count = 0\n",
    "    for index in range(len(partitions)):\n",
    "        new_v = vector(GF(2), v[count:count+partitions[index]])\n",
    "        new_m = vector(GF(2), m[count:count+partitions[index]])\n",
    "        dot_product = new_m.dot_product(new_v)\n",
    "        new_entry = [new_v, dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "        count += partitions[index]\n",
    "        \n",
    "    return sigmas \n",
    "\n",
    "\n",
    "# Input: measurement vector v ϵ ZZ^k\\{0}, Message m ϵ ZZ(2)^k and Partitions Array, that v divides into v = v_1|v_2|....|v_x by giving the cardinalities of v_i  \n",
    "# Output: amount_of_hints sigma_i = m * v_i^t as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_variant_v_m_over_Z(k, v, m, partitions):\n",
    "    if sum(partitions) != k:\n",
    "        return \"v and m cannot be divided into the stated partitions!\"\n",
    "    \n",
    "    sigmas = []\n",
    "    count = 0\n",
    "    for index in range(len(partitions)):\n",
    "        new_v = vector(GF(2), v[count:count+partitions[index]])\n",
    "        new_m = vector(GF(2), m[count:count+partitions[index]])\n",
    "        dot_product = 0\n",
    "        for index2 in range(len(new_v)):\n",
    "            if new_v[index2] == new_m[index2] == 1:\n",
    "                dot_product += 1 \n",
    "        \n",
    "        new_entry = [new_v, dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "        count += partitions[index]\n",
    "        \n",
    "    return sigmas\n",
    "\n",
    "\n",
    "\n",
    "# Input: measurement vector v ϵ GF(2)^n\\{0}, Error e ϵ GF(2)^n and Partitions Array, that v divides into v = v_1|v_2|....|v_x by giving the cardinalities of v_i    \n",
    "# Output: amount of partitions sigma_i = e_i * v_i^t as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_variant_blocks_v_e(n, v, e, partitions):\n",
    "    if sum(partitions) != n:\n",
    "        return \"v and m cannot be divided into the stated partitions!\"\n",
    "\n",
    "    sigmas = []\n",
    "    count = 0 \n",
    "    for index in range(len(partitions)):\n",
    "        new_v = vector(GF(2), v[count:count+partitions[index]])\n",
    "        new_e = vector(GF(2), e[count:count+partitions[index]])\n",
    "        dot_product = new_e.dot_product(new_v)\n",
    "        new_entry = [new_v, dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "        count += partitions[index]\n",
    "        \n",
    "    return sigmas \n",
    "                       \n",
    "\n",
    "\n",
    "# Input: amount_of_hints Measurement Vector v_i ϵ ZZ(2)^n\\{0} as Array v = [v_1, v_2, v_3, ...] and Error e ϵ ZZ^n\n",
    "# Output: amount_of_hints sigma_i = e * v_i^t as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def hint_variant_v_e_over_Z(n, v, e, partitions):\n",
    "    if sum(partitions) != n:\n",
    "        return \"v and m cannot be divided into the stated partitions!\"\n",
    "\n",
    "    sigmas = []\n",
    "    count = 0 \n",
    "    for index in range(len(partitions)):\n",
    "        new_v = vector(GF(2), v[count:count+partitions[index]])\n",
    "        new_e = vector(GF(2), e[count:count+partitions[index]])\n",
    "        dot_product = 0\n",
    "        for index2 in range(len(new_v)):\n",
    "            if new_v[index2] == new_e[index2] == 1:\n",
    "                dot_product += 1\n",
    "                \n",
    "        new_entry = [new_v, dot_product]\n",
    "        sigmas.append(new_entry)\n",
    "        count += partitions[index]\n",
    "        \n",
    "    return sigmas \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "# --- USAGE --- #                 \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "amount_partitions = 4\n",
    "v = generate_linearly_independent_v(k, 1)\n",
    "partitions = create_random_partition(amount_partitions, k)\n",
    "variant_1 = hint_variant_blocks_v_m(k, v[0], m, partitions)\n",
    "\n",
    "print(\"HINT VARIANT TYPE SIX WITH CONTINOUS RANDOM SIZE BLOCKS OVER GF(2):\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", variant_1[index][0], \"\\n>> Value:\", variant_1[index][1])\n",
    "\n",
    "variant_2 = hint_variant_v_m_over_Z(k, v[0], m, partitions)\n",
    "print(\"\\nHINT VARIANT TYPE SIX WITH CONTINOUS RANDOM SIZE BLOCKS OVER ZZ:\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", variant_2[index][0], \"\\n>> Value:\", variant_2[index][1])\n",
    "    \n",
    "    \n",
    "v = generate_linearly_independent_v(n, 1)    \n",
    "partitions = create_random_partition(amount_partitions, n)\n",
    "variant_3 = hint_variant_blocks_v_e(n, v[0], e, partitions)\n",
    "print(\"\\nHINT VARIANT TYPE FOUR WITH CONTINOUS RANDOM SIZE BLOCKS OVER GF(2):\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", variant_3[index][0], \"\\n>> Value:\", variant_3[index][1])\n",
    "    \n",
    "variant_4 = hint_variant_v_e_over_Z(n, v[0], e, partitions)\n",
    "print(\"\\nHINT VARIANT TYPE FOUR WITH CONTINOUS RANDOM SIZE BLOCKS OVER ZZ:\")\n",
    "for index in range(amount_partitions):\n",
    "    print(index, \":\", variant_4[index][0], \"\\n>> Value:\", variant_4[index][1])  \n",
    "    \n",
    "cardinalities = create_equal_partition(amount_partitions, n)    \n",
    "print(\"\\nTHERE IS ALSO THE POSSIBILITY TO CREATE PARTITIONS OF ALMOST EQUAL SIZE:\", cardinalities)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- APPROXIMATED HINTS --- #\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "# Input: amount_of_hints Measurement Vector v_i ϵ GF(2)^k\\{0} as Array v = [v_1, v_2, v_3, ...], Message m ϵ GF(2)^k and probability_epsilon in [0; 1/2(\n",
    "# Output: amount_of_hints sigma_i = m * v_i^t + epsilon mod 2 as Array sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def approximated_hints_v_m(k, v, m, amount_of_hints, probability_epsilon):\n",
    "    if len(v) != amount_of_hints:\n",
    "        return \"Amount of Hints has to match with the amount of Measurement Vectors in v!\"\n",
    "    \n",
    "    if probability_epsilon >= 0.5:\n",
    "        return \"probability_epsilon has to be in [0; 1/2(!\"\n",
    "    \n",
    "    approximation_prob = int(probability_epsilon * 10e9)\n",
    "    \n",
    "    sigmas = []\n",
    "    for index in range(amount_of_hints):\n",
    "        current_v = v[index]\n",
    "        epsilon = 0\n",
    "        \n",
    "        check = np.random.randint(0,10e9)\n",
    "        if check <= approximation_prob:\n",
    "            epsilon = 1\n",
    "        \n",
    "        dot_product = m.dot_product(current_v)\n",
    "        new_entry = dot_product + epsilon\n",
    "        sigmas.append([current_v, new_entry])\n",
    "\n",
    "    return sigmas  \n",
    "\n",
    "\n",
    "\n",
    "# Input: amount_of_hints Measurement Vector v_i ϵ GF(2)^n\\{0} as Array v = [v_1, v_2, v_3, ...], Message e ϵ GF(2)^n and probability_epsilon in [0; 1/2(\n",
    "# Output: amount_of_hints sigma_i = e * v_i^t + epsilon mod 2 as Vector sigmas = [[v_1, sigma_1], [v_2, sigma_2], ...]\n",
    "def approximated_hints_v_e(n, v, e, amount_of_hints, probability_epsilon):\n",
    "    if len(v) != amount_of_hints:\n",
    "        return \"Amount of Hints has to match with the amount of Measurement Vectors in v!\"\n",
    "    \n",
    "    if probability_epsilon >= 0.5:\n",
    "        return \"probability_epsilon has to be in [0; 1/2(!\"\n",
    "    \n",
    "    approximation_prob = int(probability_epsilon * 10e9)\n",
    "    \n",
    "    sigmas = []\n",
    "    for index in range(amount_of_hints):\n",
    "        current_v = v[index]\n",
    "        epsilon = 0\n",
    "        \n",
    "        check = np.random.randint(0,10e9)\n",
    "        if check <= approximation_prob:\n",
    "            epsilon = 1\n",
    "        \n",
    "        dot_product = e.dot_product(current_v)\n",
    "        new_entry = dot_product + epsilon\n",
    "        sigmas.append([current_v, new_entry])\n",
    "\n",
    "    return sigmas  \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "# --- USAGE --- # \n",
    "n = 237\n",
    "k = 124\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "\n",
    "amount_of_hints = 3\n",
    "probability_epsilon = 0.3\n",
    "\n",
    "v = generate_linearly_independent_v(k, amount_of_hints)\n",
    "approximated_hints_1 = approximated_hints_v_m(k, v, m, amount_of_hints, probability_epsilon)\n",
    "print(\"WE RECEIVE APPROXIMATED THE INNER PRODUCTS BETWEEN THE V_I AND M:\")\n",
    "for index in range(amount_of_hints):\n",
    "    print(index, \":\", approximated_hints_1[index][0], \"\\n>> Value:\", approximated_hints_1[index][1])\n",
    "\n",
    "v = generate_linearly_independent_v(n, amount_of_hints)\n",
    "print(\"\\nWE RECEIVE APPROXIMATED THE INNER PRODUCTS BETWEEN THE V_I AND E:\")\n",
    "approximated_hints_2 = approximated_hints_v_e(n, v, e, amount_of_hints, probability_epsilon)\n",
    "for index in range(amount_of_hints):\n",
    "    print(index, \":\", approximated_hints_2[index][0], \"\\n>> Value:\", approximated_hints_2[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- BKW ALGORITHM FOR APPROXIMATED HINTS --- #\n",
    "\n",
    "import numpy as np \n",
    "import copy\n",
    "import operator\n",
    "\n",
    "# Input: The Length of the vectors x_i, the propability, that epsilon is 1, so the label has an error and e, m and the flag_e\n",
    "#        >> e and m are only inputs for the generation of x_i's; if flag_e is set we generate x_i, then we do the algorithm for error e otherwise for m \n",
    "# Output: The Error e or the message m \n",
    "# Disclamer: One Iteration of the BKW has sub-exponential runtime and we need c^sqrt(length_vector) many iterations for solving the LWE Problem!\n",
    "#           >> For this we have to request a lot of x_i through the Oracle \n",
    "def bkw_algorithm(length_vector, probability_epsilon, e, m, flag_e):\n",
    "    if probability_epsilon >= 0.5:\n",
    "        return \"The probability of epsilon has to be in [0; 1/2(!\"\n",
    "    \n",
    "    length_m = int(1/2 * log(length_vector, 2) * 2^(2 * length_vector / log(length_vector, 2)))\n",
    "    # Edge Case: When a is 1 than the BKW Algorithm does not make any sense since we have only one block and the partial Gaussian Elimination\n",
    "    #            becomes the \"normal\" Gaussian Elimination, because of this we set a in this case to a minimum of 2!\n",
    "    a = int(1/2 * log(length_vector, 2))\n",
    "    if a == 1:\n",
    "        a += 1\n",
    "        \n",
    "    c = 1 - 2 * probability_epsilon\n",
    "    required_amount_sampels = 10 * ceil(c^(-2^a))\n",
    "    lowest_amount_samples = 0\n",
    "    majority_decision = []\n",
    "    for index in range(length_vector):\n",
    "        majority_decision.append([0,0])\n",
    "    majority_decision = np.array_split(majority_decision, a)\n",
    "    \n",
    "    while lowest_amount_samples < required_amount_sampels:\n",
    "        # Requesting m measurement vectors v from the oracle \n",
    "        vectors = []\n",
    "        for index in range(length_m):\n",
    "            amount_ones = np.random.randint(1, length_vector)\n",
    "            sample_vector = [0]*(length_vector-amount_ones) + [1]*amount_ones\n",
    "            new_vector = vector(GF(2), sample_vector)\n",
    "            shuffle(new_vector)\n",
    "            vectors.append(new_vector)\n",
    "                \n",
    "        if flag_e:\n",
    "            sigmas = approximated_hints_v_e(length_vector, vectors, e, length_m, probability_epsilon)\n",
    "        else:\n",
    "            sigmas = approximated_hints_v_m(length_vector, vectors, m, length_m, probability_epsilon)\n",
    "        \n",
    "        # Step 1 of the BKW Algorithm\n",
    "        indices = list(np.arange(length_vector))\n",
    "        indices_b = np.array_split(indices, a)\n",
    "        splitted_v = []\n",
    "        \n",
    "        # Splitting already all v in the blocks defined by a and b \n",
    "        for index in range(len(sigmas)):\n",
    "            current_v = sigmas[index][0]\n",
    "            current_splitted_v = []\n",
    "            for indices in indices_b:\n",
    "                element_current_splitted_v = []\n",
    "                for inner_index in indices:\n",
    "                    element_current_splitted_v.append(current_v[inner_index])\n",
    "                current_splitted_v.append(vector(GF(2), element_current_splitted_v))\n",
    "                        \n",
    "            current_splitted_v.append(sigmas[index][1])\n",
    "            splitted_v.append(current_splitted_v)\n",
    "  \n",
    "        # Step 2 of the BKW Algorithm \n",
    "        # Doing this step analogous for all blocks \n",
    "        for target_block in range(a):\n",
    "            offset_majority_decision = 0 \n",
    "            clon_splitted_v = splitted_v\n",
    "            # Doing the blockwise gaussian operations for the corresponding target block in this loop \n",
    "            for gauss_blocks in range(a):\n",
    "                if target_block == gauss_blocks:\n",
    "                    continue \n",
    "                \n",
    "                # Sorting the vectors by the corresponding blocks\n",
    "                clon_splitted_v = sorted(clon_splitted_v, key=operator.itemgetter(gauss_blocks))\n",
    "                \n",
    "                # Doing the gaussian step, so finding all same vectors; adding the first vector to the other ones\n",
    "                # that are the same and delete the first vector from the matrix\n",
    "                \n",
    "                # Precomputations for the gaussian elimination \n",
    "                same_indices = []\n",
    "                amount_same_indices = [0]*len(clon_splitted_v)\n",
    "                count = 0 \n",
    "\n",
    "                for index in range(len(clon_splitted_v)):\n",
    "                    if index == 0:\n",
    "                        same_indices.append(count)\n",
    "                        amount_same_indices[count] += 1 \n",
    "                        continue \n",
    "                        \n",
    "                    if clon_splitted_v[index][gauss_blocks] == clon_splitted_v[index-1][gauss_blocks]:\n",
    "                        same_indices.append(count)\n",
    "                        amount_same_indices[count] += 1 \n",
    "                    else:\n",
    "                        count += 1 \n",
    "                        same_indices.append(count)\n",
    "                        amount_same_indices[count] += 1 \n",
    "                        \n",
    "                amount_same_indices = amount_same_indices[0:count+1]\n",
    "                \n",
    "                cluster_offset = 0\n",
    "\n",
    "                # Doing the gaussian elimination \n",
    "                for amount_same in amount_same_indices:\n",
    "                    # Case: Only one vector of this type in the cluster of the block \n",
    "                    if amount_same == 1:\n",
    "                        cluster_offset += amount_same\n",
    "                        continue\n",
    "                        \n",
    "                    # Case: We have more than one vector in a cluster, so do the gaussian elimination\n",
    "                    for index in range(cluster_offset+1, cluster_offset+amount_same):\n",
    "                        new_entry = []\n",
    "                        for element in range(len(splitted_v[index])):\n",
    "                            value = clon_splitted_v[cluster_offset][element] + clon_splitted_v[index][element]\n",
    "                            new_entry.append(value)\n",
    "                            \n",
    "                        clon_splitted_v[index] = new_entry\n",
    "                    \n",
    "                    clon_splitted_v[cluster_offset] = None \n",
    "                    cluster_offset += amount_same\n",
    "                    \n",
    "                # Deleting the first vector out of the cluster; The deleted rows were set to NONE in the previous step  \n",
    "                resulting_clon_splitted_v = []\n",
    "                for index in range(len(clon_splitted_v)):\n",
    "                    if clon_splitted_v[index] == None:\n",
    "                        continue\n",
    "                    resulting_clon_splitted_v.append(clon_splitted_v[index])\n",
    "                    \n",
    "\n",
    "            # Step 3 of the BKW Algorithm\n",
    "            # Searching for unit vectors in the corresponding block \n",
    "            for index in range(len(resulting_clon_splitted_v)):\n",
    "                possible_unit_vector = resulting_clon_splitted_v[index][target_block]\n",
    "                # Case: Current Row in the Matrix corresponds to a unit vector \n",
    "                if possible_unit_vector.hamming_weight() == 1:\n",
    "                    for unit_vector in range(len(possible_unit_vector)):\n",
    "                        if possible_unit_vector[unit_vector] == 1:\n",
    "                            # Updating the amount of found labels for this unit vector\n",
    "                            learned_label = resulting_clon_splitted_v[index][len(resulting_clon_splitted_v[index])-1]\n",
    "                            if learned_label == 0:\n",
    "                                majority_decision[target_block][unit_vector][0] += 1 \n",
    "                            else:\n",
    "                                majority_decision[target_block][unit_vector][1] += 1\n",
    "\n",
    "        \n",
    "        # Updating the amount of the lowest_amount_samples \n",
    "        lowest = sum(majority_decision[0][0])\n",
    "        for index in range(len(majority_decision)):\n",
    "            for index2 in range(len(majority_decision[index])):\n",
    "                \n",
    "                if sum(majority_decision[index][index2]) < lowest:\n",
    "                    lowest = sum(majority_decision[index][index2])\n",
    "        \n",
    "        lowest_amount_samples = lowest\n",
    "    \n",
    "    # Creating the Output Vector via Majority Decision\n",
    "    output = []\n",
    "    for index in range(len(majority_decision)):\n",
    "        for index2 in range(len(majority_decision[index])):\n",
    "            if majority_decision[index][index2][0] > majority_decision[index][index2][1]:\n",
    "                output.append(0)\n",
    "            else:\n",
    "                output.append(1)\n",
    "                \n",
    "    return vector(GF(2), output)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- USAGE --- # \n",
    "n = 27\n",
    "k = 15\n",
    "w = int(approximated_packing_radius(n,k)/2)\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "probability_epsilon = 0.1\n",
    "\n",
    "calculated_e = bkw_algorithm(n, probability_epsilon, e, m, True)\n",
    "if calculated_e == e:\n",
    "    print(\"BKW SUCCESFULLY RECOVERED ERROR E!\")\n",
    "    \n",
    "calculated_m = bkw_algorithm(k, probability_epsilon, e, m, False)\n",
    "if calculated_m == m:\n",
    "    print(\"BKW SUCCESSFULLY RECOVERED MESSAGE M!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- LIST DECODING FOR THE SYNDROME DECODING PROBLEM --- #\n",
    "\n",
    "# Input: Dimensions n, k and the Hamming Weight of the Error wt_H(e) = w\n",
    "# Output: Amount possible Solutions\n",
    "def estimating_amount_solutions(n, k, w):\n",
    "    p = float(w/n)\n",
    "    R = float(k/n)\n",
    "    epsilon = 1 - binary_entropy(p) - R\n",
    "    \n",
    "    if epsilon <= 0:\n",
    "        return \"ERROR!\"\n",
    "    \n",
    "    size_List = floor(binary_entropy(p)/epsilon + 2)\n",
    "    return size_List\n",
    "\n",
    "\n",
    "\n",
    "# Input: Dimensions n, k and the Hamming Weight of the Error wt_H(e) = w \n",
    "# Output: Runtime Complexity List Decoding Prange to the Base of 2 \n",
    "def logarithmic_runtime_complexity_list_decoding_prange(n, k, w):\n",
    "    size_List = estimating_amount_solutions(n, k, w)\n",
    "    return ceil((binary_entropy(w/n) * n - binary_entropy(w/(n-k)) * (n-k)) + log(size_List, 2))\n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n,k and w(e)= w \n",
    "# Output: Message m \n",
    "def isd_primal_message_prange(G, r, n, k, w):\n",
    "    success = False\n",
    "    \n",
    "    while not success:\n",
    "        I = sorted(random.sample(list(range(n)), k))\n",
    "    \n",
    "        if G[:, I].rank() != k:\n",
    "            continue \n",
    "\n",
    "        G_I = G[:, I]\n",
    "        G_I_inverse = G_I.inverse()\n",
    "        \n",
    "        r_modified = vector(GF(2), k)\n",
    "        counter = 0\n",
    "        for value in I:\n",
    "            r_modified[counter] = r[value]\n",
    "            counter += 1 \n",
    "            \n",
    "        G_new = G_I_inverse * G\n",
    "        calculated_e = r + r_modified * G_new\n",
    "                \n",
    "        if calculated_e.hamming_weight() == w:\n",
    "            return r_modified * G_I\n",
    "\n",
    "\n",
    "        \n",
    "# Input: Generator Matrix G, Errornous Codeword r = mG + e, Dimensions n,k, w(e) = w and the size of the list l\n",
    "# Output: List of Potential Error e's\n",
    "def list_decoding_primal_prange(G, r, n, k, w, l):\n",
    "    L = []\n",
    "    while len(L) != l:\n",
    "        new_entry = isd_primal_prange(G, r, n, k, w)\n",
    "        if new_entry not in L:\n",
    "            L.append(new_entry)\n",
    "        print(\"CURRENTLY WE HAVE\", len(L), \"SOLUTIONS.\")\n",
    "        \n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "# Input: Generator Matrix G, Generator Matrix with some columns deleted called G_small, Errornous Codeword r = mG + e,\n",
    "#        Errornous Codeword with deleted indices like G_small called r_small, Dimensions n, n_small, k, wt_H(e) = w,\n",
    "#        Hamming Weight of the remaining e called w_small and l, where l is the number of possible solutions.\n",
    "# Output: Unique Solution for the bigger SDP Instance, defined by G and r \n",
    "def list_decoding_primal_prange_modifying_G(G, G_small, r, r_small, n, n_small, k, w, w_small, l):\n",
    "    big_success = False\n",
    "    unique_solutions = []\n",
    "    \n",
    "    while not big_success:\n",
    "        if len(unique_solutions) == l:\n",
    "            return None\n",
    "        \n",
    "        message = isd_primal_message_prange(G_small, r_small, n_small, k, w_small)\n",
    "        \n",
    "        if message not in unique_solutions:\n",
    "            unique_solutions.append(message)\n",
    "        \n",
    "        potential_e = r + message * G\n",
    "        if potential_e.hamming_weight() == w:\n",
    "            return potential_e\n",
    "        \n",
    "        \n",
    "n = 117\n",
    "k = 90\n",
    "w = int(approximated_packing_radius(n,k) + 4) \n",
    "\n",
    "P, G, s, r, e, m = generate_syndrome_decoding_problem(n, k, w)\n",
    "print(\"POSSIBLE FIVE SOLUTIONS ARE FOR THE CURRENT SYNDROME DECODING PROBLEM:\\n\", list_decoding_primal_prange(G, r, n, k, w, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# --- ATTEMPTS GUESSING ERROR-LOCATIONS WITHOUT THE TRANSFORMATIONS --- #\n",
    "\n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "\n",
    "print(\"WHEN WE ARE GUESSING ERROR OR ERROR-FREE LOCATIONS WITHOUT APPLYING THE TRANSFORMATION. WE TRY TO REDUCE THE WEIGHT W, BUT WE CANNOT REDUCE THE DIMENSION K\")\n",
    "print(\"SINCE WE ARE DELETING A COLUMN, MOST PROBABLY WE DIMENSION K STAYS THE SAME. THE PARAMETER N SHOULD ALSO BE THE SAME, BECAUSE\")\n",
    "print(\"REDUCING ONLY THE PARAMETER N CAUSES A LARGER RUNTIME! SO THE CHANGE OF THE SDP INSTANCE IS BY CHANGING THE ERROR LOCATIONS\")\n",
    "print(\"FOR GUESSING ONE ERROR LOCATION WE RECEIVE FOLLOWING EXPECTED RUNTIME:\\n\")\n",
    "\n",
    "print(round(w/n * logarithmic_runtime_complexity_prange(n, k, int(w-1)) + (1 - w/n) * logarithmic_runtime_complexity_list_decoding_prange(n, k, int(w+1))), \"TO THE BASE OF 2; INCLUDING THE LIST DECODING FOR GUESSING WRONGLY THE ERROR LOCATION\")\n",
    "print(\">> SO GUESSING ERROR LOCATIONS WITHOUT THE TRANSFORMATION DOES NOT MAKE ANY SENSE, SINCE THE RUNTIME IS WORSE THAN THE NORMAL PRANGE!\\n\")\n",
    "\n",
    "print(\"GUESSING ERROR-FREE LOCATIONS DOES NOT MAKE SENSE IN THIS SETUP SINCE WE ONLY CAN FIX ONE INDEX OF THE INFORMATION-SET BUT THE\")\n",
    "print(\"SUCCESS PROBABILITY OF PRANGE IN THE GOOD CASE DOESN'T CHANGE AND IN THE BAD CASE THE INFORMATION-SET IS NEVER ERROR-FREE AND\")\n",
    "print(\"PRANGE'S ISD IS NOT SUCCESSFUL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# --- ATTEMPTS GUESSING ERROR-LOCATIONS WITH TRANSFORMATIONS --- #\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# Input: Dimensions n, k and w = wt_H(e)\n",
    "# Output: Displaying the Resulting Runtime trying Guessing Error Locations\n",
    "def distribution_guessing_error_locations(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    iterations = []\n",
    "    overall_runtime = []\n",
    "    \n",
    "    for amount_of_hints in range(1, w+1):\n",
    "        current_probability = hypergeometric_distribution(n, w, amount_of_hints, amount_of_hints)\n",
    "        x.append(amount_of_hints)\n",
    "        y.append(float(current_probability))\n",
    "        z.append(logarithmic_runtime_complexity_prange(int(n-amount_of_hints), int(k-amount_of_hints), int(w-amount_of_hints)))\n",
    "        iterations.append(round(log(round(1/current_probability), 2)))\n",
    "        overall_runtime.append(round(log(round(1/current_probability), 2)) + logarithmic_runtime_complexity_prange(int(n-amount_of_hints), int(k-amount_of_hints), int(w-amount_of_hints)))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    iterations = np.array(iterations)\n",
    "    overall_runtime = np.array(overall_runtime)\n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('Amount of Guessed Errornous Entries')\n",
    "    plt.ylabel('Success Probability Guessing only Errornous Entries')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,z)\n",
    "    plt.xlabel('Amount of Guessed Errornous Entries')\n",
    "    plt.ylabel('Prange ISD Runtime to the base of 2 simplified SDP')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,iterations)\n",
    "    plt.xlabel('Amount of Guessed Errornous Entries')\n",
    "    plt.ylabel('Estimated Amounts of Iterations to the base of 2')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,overall_runtime)\n",
    "    plt.axhline(y=logarithmic_runtime_complexity_prange(n, k, w), color='r', linestyle='-', label='Runtime Prange actual SDP Instance')\n",
    "    plt.xlabel('Amount of Guessed Errornous Entries')\n",
    "    plt.ylabel('Overall Runtime until Success to the base of 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# --- USAGE --- # \n",
    "\n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "\n",
    "print(\"NOTE: INFORMATION-SET DECODING ALGORITHMS ARE BY FAR THE MOST EFFICIENT WAY OF SOLVING THE SYNDROME DECODING PROBLEM\")\n",
    "print(\"REPRESENTATIVELY WE USE THE RUNTIME COMPLEXITY OF PRANGE'S PRIMAL ISD\")\n",
    "\n",
    "print(\"\\nWHEN GUESSING ERROR LOCATIONS OF E, WE SIMPLY INVERT THE LOCATIONS IN THE ERRORNOUS CODEWORD r\")\n",
    "print(\"WHEN WE GUESSING CORRECTLY THE ERROR E WE RECEIVE AS NEW RUNTIME TO THE BASE OF 2:\", logarithmic_runtime_complexity_prange(n-1, k-1, w-1))\n",
    "print(\"THE PROBABILITY OF GUESSING ONLY ONE LOCATION CORRECTLY IS:\", float(w/n))\n",
    "print(\"OTHERWISE WE DESTROY THE INSTANCE WITH A PROBABILITY OF\", 1 - float(w/n))\n",
    "print(\"BUT WE NEED\", int(round(1/float(w/n))), \"Iterations in the Average Case to find a Error Location.\")\n",
    "print(\"SO OVERALL WE GET A ESTIMATED RUNTIME OF\", logarithmic_runtime_complexity_prange(n-1, k-1, w-1) + int(round(log(round(1/float(w/n)) , 2))), \"TO THE BASE OF 2\")\n",
    "print(\">> GUESSING ONE ERROR LOCATION IS WORSE THAN NORMAL PRANGE!\")\n",
    "\n",
    "print(\"\\nNOW WE WILL HAVE A LOOK ON GUESSING SEVERAL ERROR-FREE LOCATIONS OF E AND APPLYING THE TRANSFORMATION OF HINT TYPE ONE/TWO:\\n\")\n",
    "distribution_guessing_error_locations(n, k, w)\n",
    "\n",
    "print(\"\\n>> SO EVEN WHEN THE TERMINATE EVERY PRANGE INSTANCE AFTER THE ESTIMATED AMOUNT OF ITERATIONS, WE GET A RUNTIME THAT IS WORSE THAN THE NORMAL PRANGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# --- GUESSING ERROR-FREE LOCATIONS WITH TRANSFORMATIONS --- #\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# Input: Dimensions n, k and w = wt_H(e)\n",
    "# Output: Displaying the Resulting Runtime trying Guessing Error-Free Locations\n",
    "def distribution_guessing_error_free_locations(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    iterations = []\n",
    "    overall_runtime = []\n",
    "    \n",
    "    amount_of_hints = 1 \n",
    "    current_probability = 1 \n",
    "    while True:\n",
    "        if current_probability < 0.01:\n",
    "            break\n",
    "            \n",
    "        current_probability = hypergeometric_distribution(n, n-w, amount_of_hints, amount_of_hints)\n",
    "        x.append(amount_of_hints)\n",
    "        y.append(float(current_probability))\n",
    "        z.append(logarithmic_runtime_complexity_prange(int(n-amount_of_hints), int(k-amount_of_hints), w))\n",
    "        iterations.append(round(1/current_probability))\n",
    "        overall_runtime.append(round(log(round(1/current_probability), 2)) + logarithmic_runtime_complexity_prange(int(n-amount_of_hints), int(k-amount_of_hints), w))\n",
    "        amount_of_hints = amount_of_hints + 1\n",
    "    \n",
    "    bound = 0\n",
    "    for index in range(len(x)):\n",
    "        if y[index] < 0.5:\n",
    "            bound = x[index]\n",
    "            break\n",
    "    \n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    iterations = np.array(iterations)\n",
    "    overall_runtime = np.array(overall_runtime)\n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('Amount of Guessed Error-Free Entries')\n",
    "    plt.ylabel('Success Probability Guessing only Error-Free Entries')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,z)\n",
    "    plt.xlabel('Amount of Guessed Error-Free Entries')\n",
    "    plt.ylabel('Prange ISD Runtime to the base of 2 simplified SDP')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,iterations)\n",
    "    plt.xlabel('Amount of Guessed Error-Free Entries')\n",
    "    plt.ylabel('Estimated Amounts of Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,overall_runtime)\n",
    "    plt.xlabel('Amount of Guessed Error-Free Entries')\n",
    "    plt.ylabel('Overall Runtime until Success to the base of 2')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# --- USAGE --- #\n",
    "\n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "\n",
    "print(\"NOTE: INFORMATION-SET DECODING ALGORITHMS ARE BY FAR THE MOST EFFICIENT WAY OF SOLVING THE SYNDROME DECODING PROBLEM\")\n",
    "print(\"REPRESENTATIVELY WE USE THE RUNTIME COMPLEXITY OF PRANGE'S PRIMAL ISD\")\n",
    "\n",
    "print(\"\\nWHEN GUESSING ERROR-FREE LOCATIONS OF E, WE SIMPLY INVERT THE LOCATIONS IN THE ERRORNOUS CODEWORD r\")\n",
    "print(\"WHEN WE GUESSING CORRECTLY THE ERROR-LOCATION E WE RECEIVE AS NEW RUNTIME TO THE BASE OF 2:\", logarithmic_runtime_complexity_prange(n-1, k-1, w))\n",
    "print(\"THE PROBABILITY OF GUESSING ONLY ONE LOCATION CORRECTLY IS:\", float(1 - w/n))\n",
    "print(\"OTHERWISE WE DESTROY THE INSTANCE WITH A PROBABILITY OF\", float(w/n))\n",
    "print(\">> GUESSING ONE ERROR-FREE LOCATION IS AS COMPLEX AS NORMAL PRANGE!\")\n",
    "\n",
    "print(\"\\nNOW WE WILL HAVE A LOOK ON GUESSING SEVERAL ERROR-FREE LOCATIONS OF E AND APPLYING THE TRANSFORMATION OF HINT TYPE THREE:\\n\")\n",
    "distribution_guessing_error_free_locations(n, k, w)\n",
    "\n",
    "print(\"\\n>> SO EVEN WHEN THE TERMINATE EVERY PRANGE INSTANCE AFTER THE ESTIMATED AMOUNT OF ITERATIONS, WE GET A RUNTIME THAT IS EQUAL OR WORSE THAN THE NORMAL PRANGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": false
   },
   "outputs": [
   ],
   "source": [
    "# --- ATTEMPTS GUESSING SUB-ERRORS AND THEIR HAMMING WEIGHT --- #\n",
    "\n",
    "import numpy as np \n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Input: n, k, wt_H(e) = w and the amount of hints\n",
    "# Output: The packing radius when guessing amount of hints random locations e and deleting these columns\n",
    "def determine_unique_decoding(n, k, w, amount_of_hints):\n",
    "    for index in range(w, 0, -1):\n",
    "        if is_unique_decoding(int(n-amount_of_hints), k, index):\n",
    "            return index \n",
    "\n",
    "        \n",
    "        \n",
    "# Input: Dimensions n, k and the wt_H(e) = w \n",
    "# Output: Displaying the Distribution of the Packing Radius, when changing the n\n",
    "def distribution_approximated_packing_radius(n, k, w):\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0, floor((n-k)/2)):\n",
    "        x.append(n-index)\n",
    "        y.append(approximated_packing_radius(n-index,k))\n",
    "    \n",
    "    x = x[::-1]\n",
    "    y = y[::-1]\n",
    "    biggest_n = 0\n",
    "    for index in range(len(y)):\n",
    "        if y[index] <= w:\n",
    "            biggest_n = x[index]\n",
    "            \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.xticks([3100, 3150, 3200, 3250, 3300, 3350, 3400, 3440, biggest_n])\n",
    "    plt.plot(x,y)\n",
    "    plt.axvline(x = biggest_n, color = 'r', linestyle = '--', label='Border Unique Decoding')\n",
    "    plt.title('Estimating Packing Radius of the Subcode')\n",
    "    plt.xlabel('Parameter n')\n",
    "    plt.ylabel('Packing Radius Subcode')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "# Input: n, k, wt_H(e) = w and the amount of hints\n",
    "# Output: Returning the most common values for the remaining e, their probabilities and the probability of all hamming weights\n",
    "def most_common_hamming_weights(n, k, w, amount_of_hints):\n",
    "    indices = []\n",
    "    values = []\n",
    "    for index in range(w+1):\n",
    "        current_probability = hypergeometric_distribution(n, w, amount_of_hints, index)\n",
    "        indices.append(w - index)\n",
    "        values.append(current_probability)\n",
    "    \n",
    "    indices = indices[::-1]\n",
    "    values = values[::-1]    \n",
    "    \n",
    "    most_common_indices = []\n",
    "    most_common_values = []\n",
    "    for index in indices:\n",
    "        if float(values[index]) >= 0.001:\n",
    "            most_common_indices.append(index)\n",
    "            most_common_values.append(float(values[index]))\n",
    "    \n",
    "    return most_common_indices, most_common_values, values        \n",
    "\n",
    "    \n",
    "    \n",
    "# Input: n, k, wt_H(e) = w and the amount of hints\n",
    "# Output: Displaying the amount of expected Hamming Weight and the Probability of the remaining e\n",
    "def distribution_guessing_density_remaining_error(n, k, w, amount_of_hints):\n",
    "    most_common_indices, most_common_values, values = most_common_hamming_weights(n, k, w, amount_of_hints)\n",
    "    unique_decoding = determine_unique_decoding(n, k, w, amount_of_hints)\n",
    "    \n",
    "    x = np.array(most_common_indices)\n",
    "    y = np.array(most_common_values)\n",
    "    plt.bar(x,y) \n",
    "    plt.axvline(x = unique_decoding + 0.5, color = 'r', linestyle = '--', label='Border Unique Decoding')\n",
    "    plt.title('Probabilities of Hamming Weights in the Remaining e')\n",
    "    plt.xlabel('Hamming Weight')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"THE PROBABILITY THAT THE REMAINING ERROR IS UNIQUELY DECODABLE IS:\", float(sum(values[:unique_decoding+1])))\n",
    "    \n",
    "\n",
    "    \n",
    "# Input: n, k, wt_H(e) = w and the amount of hints\n",
    "# Output: Resulting Runtime Complexities of the Modified Primal Prange's ISD for the new Instance   \n",
    "def complexity_unique_decodable_remaining_e(n, k, w, amount_of_hints):  \n",
    "    most_common_indices, most_common_values, values = most_common_hamming_weights(n, k, w, amount_of_hints)\n",
    "    if most_common_values == []:\n",
    "        print(\">> PROBABILITY IS TO SMALL!\")\n",
    "    \n",
    "    unique_decoding = determine_unique_decoding(n, k, w, amount_of_hints)\n",
    "    x = []\n",
    "    y = []\n",
    "    expected_runtime = 0\n",
    "    \n",
    "    for index in range(len(most_common_indices)):\n",
    "        if most_common_indices[index] <= unique_decoding:\n",
    "            x.append(most_common_indices[index])\n",
    "            y.append(logarithmic_runtime_complexity_prange(int(n-amount_of_hints), k, most_common_indices[index]))\n",
    "            expected_runtime += most_common_values[index] * logarithmic_runtime_complexity_prange(int(n-amount_of_hints), k, most_common_indices[index])\n",
    "        else:\n",
    "            x.append(most_common_indices[index])\n",
    "            y.append(logarithmic_runtime_complexity_list_decoding_prange(int(n-amount_of_hints), k, most_common_indices[index]))\n",
    "            expected_runtime += most_common_values[index] * logarithmic_runtime_complexity_list_decoding_prange(int(n-amount_of_hints), k, most_common_indices[index])\n",
    "            \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.plot(x, y, 'o')\n",
    "    plt.axhline(y = logarithmic_runtime_complexity_prange(n, k, w), color = 'r', linestyle = '--', label='Complexity Normal Instance')\n",
    "    plt.axvline(x = unique_decoding, color = 'b', linestyle = '--', label='Border Unique Decoding')\n",
    "    plt.title('Runtime Complexity Modified Primal Prange including List Decoding if necessary')\n",
    "    plt.xlabel('Hamming Weight of the remaining e')\n",
    "    plt.ylabel('Complexity Modified Primal Prange to the Base of 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"THE EXPECTED RUNTIME IS THEREFORE:\", expected_runtime, \"TO THE BASE OF 2\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# --- USAGE --- # \n",
    "n = 3488\n",
    "k = 2720\n",
    "w = 64\n",
    "amount_of_hints = 50\n",
    "\n",
    "print(\"NOW WE WILL HAVE A LOOK ON THE HAMMING WEIGHT OF THE REMAINING E WHEN WE ARE DELETING RANDOMLY\", amount_of_hints, \"ERROR LOCATIONS\")\n",
    "print(\"FOR THIS PURPOSE WE HAVE TO DETERMINE WHETHER THE DECODING IS STILL UNIQUE. WE SHOWED THAT WITH PROBABILITY ALMOST 1, K DOESN'T CHANGE\")\n",
    "print(\"WHEN DELETING COLUMNS OF THE GENERATOR MATRIX G. FOR THIS REASON THE PACKING RADIUS REDUCES LIKE WE SEE IN THE FOLLOWING GRAPHIC:\")\n",
    "distribution_approximated_packing_radius(n, k, w)\n",
    "\n",
    "print(\"THE BORDER OF UNIQUE DECODING IS DETERMINE THROUGH THE ORIGINAL HAMMING DISTANCE OF E SINCE IT'S NOT SO LIKELY TO HIT ERROR POSITIONS OF E. SEE HERE:\")\n",
    "distribution_guessing_density_remaining_error(n, k, w, amount_of_hints)\n",
    "\n",
    "print(\"\\nTHE RUNTIME IS CALCULATED THROUGH THE MODIFIED PRIMAL PRANGE INFORMATION-SET DECODING ALGORITHM INCLUDING THE LIST-DECODING STEP IF NECESSARY!\")\n",
    "complexity_unique_decodable_remaining_e(n, k, w, amount_of_hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "sage-10.0",
    "--python",
    "-m",
    "sage.repl.ipython_kernel",
    "--matplotlib=inline",
    "-f",
    "{connection_file}"
   ],
   "display_name": "SageMath 10.0",
   "env": {
   },
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 1,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-10.0",
   "resource_dir": "/ext/jupyter/kernels/sage-10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}